---
title: "Melbourne Housing Market - Paper2"
subtitle: "Intro Data Science"
author: "Team 2"
date: "`r Sys.Date()`"
output:
   html_document:
     toc: true
     toc_float: true
     number_section: true
---

```{r stats_libs, include=F}
loadPkg("leaps")
loadPkg("ggplot2")
loadPkg("caret")
```



```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
knitr::opts_chunk$set(echo = F)
options(scientific=T, digits = 5) 
options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times

source("general_support.R", local = knitr::knit_global())
source("P2_support.R", local = knitr::knit_global())

```


```{r data, include=F}

train <- load_data()
test <- load_data(F)

```




```{r, include=F}

loadPkg("xtable")
loadPkg("kableExtra")
loadPkg("stringi")

xkabledply = function(modelsmmrytable, title="Table", digits = 4, pos="left", bso="striped", wide=FALSE) { 
  #' Combining base::summary, xtable, and kableExtra, to easily display model summary. 
  #' wrapper for the base::summary function on model objects
  #' Can also use as head for better display
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param modelsmmrytable This can be a generic table, a model object such as lm(), or the summary of a model object summary(lm()) 
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return HTML table for display
  #' @examples
  #' library("xtable")
  #' library("kableExtra")
  #' xkabledply( df, title="Table testing", pos="left", bso="hover" )
  #' xkabledply( ISLR::Hitters[1:5,] )
  if (wide) { modelsmmrytable <- t(modelsmmrytable) }
  modelsmmrytable %>%
    xtable() %>% 
    kable(caption = title, digits = digits) %>%
    kable_styling(bootstrap_options = bso, full_width = FALSE, position = pos)
}

xkabledplyhead = function(df, rows=5, title="Head", digits = 4, pos="left", bso="striped") { 
  xkabledply(df[1:rows, ], title, digits, pos, bso, wide=FALSE)
}

xkabledplytail = function(df, rows=5, title="Tail", digits = 4, pos="left", bso="striped") { 
  trows = nrow(df)
  xkabledply(df[ (trows-rows+1) : trows, ], title, digits, pos, bso, wide=FALSE)
}

xkablesummary = function(df, title="Table: Statistics summary.", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display numeric variable summary of dataframes. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param df The dataframe.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return The HTML summary table for display, or for knitr to process into other formats 
  #' @examples
  #' xkablesummary( faraway::ozone )
  #' xkablesummary( ISLR::Hitters, title="Five number summary", pos="left", bso="hover"  )
  
  s = summary(df) %>%
    apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% # replace all leading words
    apply( 2, function(x) stringr::str_trim(x, "right")) # trim trailing spaces left
  
  colnames(s) <- stringr::str_trim(colnames(s))
  
  if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max') 
  } else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
  
  xkabledply(s, title=title, digits = digits, pos=pos, bso=bso )
}


xkablevif = function(model, title="VIFs of the model", digits = 3, pos="left", bso="striped", wide=TRUE) { 
  #' Combining faraway::vif, xtable, and kableExtra, to easily display numeric summary of VIFs for a model. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param model The lm or compatible model object.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return The HTML summary table of the VIFs for a model for display, or for knitr to process into other formats 
  #' @examples
  #' xkablevif( lm(Salary~Hits+RBI, data=ISLR::Hitters, wide=T ) )
  
  df <- data.frame(vif(model))                            
  names(df) <- "VIF"
  
  if (wide) { df <- t(df) }
  xkabledply(df, title=title, digits = digits, pos=pos, bso=bso )    
}
```



```{r data, include=F}
# run this chunk to load data so that we all use same data for training/testing

train <- load_data()
test <- load_data(F)

```
# Introduction


## Goals
1.	Understand which attributes of a home and its sale determine final sale price
2.	Attempt to build a reasonable model for inference and/or prediction for final sale price


## Dataset

We are using the same data as our last project to predict price as both a continuous and categorical variable. We have variables describing location, construction details, and sales data attributes with approximately 13,000 observations. Some of our categorical variables have many levels, even hundreds. Likewise, some of the levels are even unique which make them impractical as predictors. We randomly selected 70% of the dataset as training and the remainder as testing. 

- Home Sales in 2017
  - Location
  - Construction
  - Sale
- Variables: 20
  - Numeric: 12
  - Categorical: 8
  
Rooms: Number of rooms
                                                                                          
Price: Price (AUS$)

Method: Method of sale - 5 categories      

Type:  House, Unit, Townhouse - 3 categories   

SellerG: Real Estate Agent - 268 categories             

Date: Date sold                                                                                 

Distance: Distance from Central Business District

Regionname: Region name - 8 categories

Propertycount: Number of properties that exist in the suburb
                                                                    
Bedroom2 : Number of Bedrooms                              
                                                        
Bathroom: Number of Bathrooms                                                                                                    

Car: Number of carspots                                                                                                   

Landsize: Land Size                             

BuildingArea: Building Size

YearBuilt: Year home built

CouncilArea: Governing council for the area - 34 categories

Lattitude, Longtitude: GPS location

Suburb: Suburb name - 314 categories                        


## Exploratory Data Analysis

We have a more in-depth exploratory data analysis in our first project. One of the more important things to note from that first analysis is that the `Price` data is log-normal which is supported by hypothesis testing. Note that we are only showing the testing set here but using the whole set just reveals the same pattern as can be seen in project one. 

```{r eda, results='markup'}
hist(test$Price)
```


## New for Project 2

We used the same `Price` categories (`Price.cuts`) that we introduced in project one. At that time, we created 10 categories of equally sized intervals of `Price` but because `Price` is log-normal and most prices are therefore in the lowest categories, we condensed the top five price category levels into the top two. This allowed us to have a reasonable number of observations in each level while having reasonably sized intervals. Although we did this in project one, we never actually modeled using this category but only used them for independence hypothesis testing. 

Also, new to this project, we imputed missing data. Luckily, we were only missing data from a few numerical variables. So, we used the mean of the suburb to impute. Unfortunately, some suburbs were unique within the dataset and in that case we simply used the mean for the entire variable. Fortunately, there were not too many cases of this.



# Linear Model Variable Selection
```{r }
loadPkg("dplyr")

# df<-data.frame(read.csv("melb_data.csv"))

df1 <- select(train,
            Price, Distance, Rooms, Bedroom2, Bathroom, Car, 
            Landsize, Lattitude, Longtitude, 
            Propertycount, BuildingArea, YearBuilt)

reg.best5 <- regsubsets(Price~. , data = df1, nvmax = 5, nbest = 1, method = "exhaustive")

plot(reg.best5, scale = "bic", main = "BIC")

mod.select <- lm(Price ~ Distance + Rooms + Lattitude + BuildingArea + YearBuilt, data=train)

p.lm <- predict(mod.select, test)

r.2.lm <- r2(p.lm, test$Price)

mse.lm <- mean((p.lm - test$Price)^2)
```

The best model is based on five variables: `Distance`, `Rooms`, `Lattitude`, `BuildingArea`, `YearBuilt` 




# Regularization: Lasso regression
## Lasso Model
```{r regularization process for data preparation, include= FALSE}
# data for trainning model
train.X <- data.matrix(train[,-which(names(train) %in% c("Price", "Price.cuts"))])
train.y <- data.matrix(log(train$Price))

# data for testing model
test.X <- data.matrix(test[,-which(names(test) %in% c("Price", "Price.cuts"))])
test.y <- data.matrix(log(test$Price))

```


```{r lasso, results='markup'}
loadPkg("glmnet")

lasso <- cv.glmnet(x = train.X, y = train.y, nfolds = 10)

plot(lasso)

```


## Coefficients

```{r coef in lasso regression}

coef(lasso, s = 'lambda.min', exact = TRUE)
min(lasso$cvm)

```

From the coefficient of lasso regression model, the variables of `Rooms`, `Type`, `Distance`, `Postcode`,  `Bathroom`, `YearBuilt`, `Lattitude`, and `Longtitude` are related with response variable `Price`.


## Lasso Results

```{r lasso fit, include=TRUE, results='markup'}
set.seed(999)
lasso_pred <- as.numeric(exp(predict(lasso, newx = test.X, s = "lambda.min"))-1)
hist(lasso_pred, main="Histogram of Lasso Predictions", xlab = "Predictions")

mse_lasso_1 = sum((lasso_pred-exp(test.y))^2)/length(test.y)

r.2.lasso_1 <- r2(lasso_pred, test$Price)
```

## Lasso Prediction

```{r lasso predict, include=TRUE, results='markup'}

plot(exp(test.y),lasso_pred,xlab="True  price",ylab="Predicted price",
     main="Prediction using Lasso regression")

text(-1,3,substitute(r^2 == r2,list(r2=cor(test.y,lasso_pred))),adj=0)
text(-1,2.7,substitute(MSE == r2,list(r2=mse_lasso_1)),adj=0)
abline(0,1)

```


## Lasso Results: Normalized

```{r predict with normalized price, include= TRUE, results='markup'}
lasso_fit = glmnet(train.X,train.y,alpha = 1,lambda = lasso$lambda.min)
pred.y = predict(lasso_fit,test.X)

mse_lasso = mean((exp(pred.y)-exp(test.y))^2)

plot(test.y,pred.y,xlab="True normalized price",ylab="Predicted normalized price",
     main="Prediction using Lasso regression")
text(-1,3,substitute(r^2 == r2,list(r2=cor(test.y,pred.y))),adj=0)
text(-1,2.7,substitute(MSE == r2,list(r2=mse_lasso)),adj=0)
abline(0,1)

r.2.lasso_2 <- r2(exp(pred.y), test$Price)
```






# Tree and Random Forest 

## Tree

Decision trees are usually an easily understood way to associate predictors with an outcome variable. The tree is composed of nodes that represent binary choices based on the dataset variables. When the predictor variable is continuous, the binary decision is a cutoff on the interval of the original variable. Unfortunately, factors with many levels may bias the decision tree to include that variable (Deng, 2011). Considering that some of our categorical variable had many levels, this is a concern for our dataset. Furthermore, the `tree()` function in R only allows variables with up to 32 levels so some variables are excluded from consideration (`SellerG`, `Suburb`, `Date`, `Postcode`, `CouncilArea`). Because of the inclusion of these many-leveled factors even with the worst offenders excluded along with the less descriptive coding of levels, another common benefit of trees is lost to us. Usually, decision trees are easily understood visually but the visualization of this tree is not very helpful. Ultimately, this method gives a middling 

An issue more specific to our dataset is the log-normal distribution of the `Price` data. When splitting the variable into categories, the top five categories were condensed into the top two. This allows each price category to have a reasonable number of observations without making the highest category start its interval at a value more reasonably and colloquially described as average. This places a large majority of the observations in the lowest two categories. Hence, the tree model is able to obtain reasonable accuracy (0.704) by only populating predictions within those lower two categories. So, the model is a bit overfitted and has no prediction ability in the higher prices. 

```{r tree, results='markup'}
loadPkg("tree")
loadPkg("pROC")


# factors < 32 cats
mod.tree <-tree(factor(Price.cuts)~., 
                data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)))
  
p <- predict(mod.tree, test, type="class")
  
acc.tree <- sum(p == test$Price.cuts)/length(p)

cm.tree <- confusionMatrix(p, test$Price.cuts)

xkabledply(cm.tree$table, title=NULL)

```





## Random Forest

### Random Forest: Regression

```{r RF_reg}
loadPkg("randomForest")

# factors < 53 cats
mod.RF <- randomForest(Price~., 
                       data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), 
                       importance=T, ntree=200)
  
p.RF_reg <- predict(mod.RF, test)
  
r.2.RF <- r2(p.RF_reg, test$Price)

mse.RF <- mean((p.RF_reg - test$Price)^2)
```

One way to minimize the overfitting of the single tree model to create many trees and take the mean of the outputs predicted from each tree. This is called a random forest model. For our project, we created a random forest that uses 200 trees and 5 variables per tree. Again, the `randomForest()` function in R limits the levels of factors at 53; so (`SellerG`, `Suburb`, `Date`, `Postcode`) are excluded. This greatly improved prediction variance explained ($R^2$ = 0.809) on the test set from previous attempts. Likewise, MSE also improved by roughly 10 times. 

### Random Forest: Categorical

```{r RF_cats, results='markup'}
# factors < 53 cats
mod.RF.cat <- randomForest(Price.cuts~., 
                           data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), 
                           importance=T, ntree=200)
  
p <- predict(mod.RF.cat, test)

acc.RF <- sum(p == test$Price.cuts, na.rm=T)/length(p)

mod.RF.cat$confusion

```

This is similar to the regression random forest but uses the mode of the predicted classes among the 200 trees. Otherwise, the setup is the same as the regression model. Although accuracy is greatly improved to 0.831, notice that accuracy for each class varies. Although predictions are placed into the top price categories, the accuracy within those classes is rather low. Most of the accuracy of the model is earned by the lower two levels.  







# k-NN
```{r knn_data_setup, include=FALSE}
#Preparing Training data
train<-train[,c(21,2,12,8,14,17,18,20,19)]
train$Rooms<-as.factor(train$Rooms)

#Creating Dummy Variables for Categorical Variables
loadPkg('fastDummies')
train <- dummy_cols(train, select_columns = 'Rooms')
train <- dummy_cols(train, select_columns = 'Regionname')

train<-train[,-c(2,9)]

#Scaling Numeric Variables
train$Car<- as.numeric(scale(train$Car, center = TRUE, scale = TRUE))
train$Distance<- as.numeric(scale(train$Distance, center = TRUE, scale = TRUE))
train$BuildingArea<- as.numeric(scale(train$BuildingArea, center = TRUE, scale = TRUE))
train$Lattitude<- as.numeric(scale(train$Lattitude, center = TRUE, scale = TRUE))
train$Longtitude<- as.numeric(scale(train$Longtitude, center = TRUE, scale = TRUE))
train$Propertycount<- as.numeric(scale(train$Propertycount, center = TRUE, scale = TRUE))


Rooms_10<-rep(0,nrow(train))
train$Rooms_10<-Rooms_10



#Preparing Testing data
test<-test[,c(21,2,12,8,14,17,18,20,19)]
test$Rooms<-as.factor(test$Rooms)

#Creating Dummy Variables for Categorical Variables
test <- dummy_cols(test, select_columns = 'Rooms')
test <- dummy_cols(test, select_columns = 'Regionname')

test<-test[,-c(2,9)]

#Scaling Numeric Variables
test$Car<- as.numeric(scale(test$Car, center = TRUE, scale = TRUE))
test$Distance<- as.numeric(scale(test$Distance, center = TRUE, scale = TRUE))
test$BuildingArea<- as.numeric(scale(test$BuildingArea, center = TRUE, scale = TRUE))
test$Lattitude<- as.numeric(scale(test$Lattitude, center = TRUE, scale = TRUE))
test$Longtitude<- as.numeric(scale(test$Longtitude, center = TRUE, scale = TRUE))
test$Propertycount<- as.numeric(scale(test$Propertycount, center = TRUE, scale = TRUE))



#Labels
trainLabels <- train[, 1]
testLabels <- test[, 1]

```



```{r knn, include=F}
loadPkg("FNN")

#Model with best total accuracy
mel_pred <- knn(train =train[,-1], test = test[,-1], cl=trainLabels, k=13, prob=T)

loadPkg("caret") 
cm = confusionMatrix(mel_pred, reference = testLabels )
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
ResultDf<-ResultDf[nrow(ResultDf)+1,]

ResultDf$k<-13
ResultDf$Total.Accuracy<-cm$overall['Accuracy']
rownames(ResultDf)<-1

```
We used a k-nearest neighbors model to build a classifier to predict housing price category (`Price.cuts`). K-nearest neighbors is a supervised classification machine learning algorithm for a categorical response variable. After exploring different numbers and combinations of variables, our final model used seven variables: `Car`, `Distance`, `Rooms`, `Region`, `Latitude`, `Longitude`, and `Property Count`. Next, we determined the optimal k value. We ran the model with k values ranging from 3 to 13 and found that 13 produced the highest accuracy. 

## Accuracy of k-NN
```{r knn_acc, results="asis"}
xkabledply(ResultDf, "Total Accuracy Summary")
```
Thus, our model is a 13-nn and it predicted housing price category with a little over 73% accuracy.


## ROC for k-NN
```{r knn_roc, results='asis'}

knn_prob=attributes(mel_pred)$prob
test$knn_prob=knn_prob
h1 <- roc(Price.cuts~knn_prob, data=test)
auc(h1)
plot(h1)
```
Next, we did an ROC-AUC to examine the quality of our classifier and found that it had an AUC score of 0.76. This a bit below the 0.8 threshold that one would typically like to see. A good classifier achieves an AUC score of 0.8 or above, so we can conclude our model produced a decent, but not great, classifier of housing price.






# Final Results

## Results for Regression Models

The BIC selection mostly confirms the continuous variables that we used in the final linear model for project one. The BIC-chosen model has a testing $R^2$ = 0.45 versus the project one model with $R^2$ = 0.44, marginally worse. Note, however, this is not directly comparable since project one did not have imputed values for missing data as we have here. When we consider the imputed values, the final model from project one has $R^2$ = 0.50 which is likely due to considering a larger subset of variables when making the model. Categorical variables were not included in the BIC selection method. Testing MSE follows a similar pattern ($2.16 \times 10^{11}$ for the imputed project one). As can be seen in the table, lasso regression also did not improve these metrics although it included a larger range of variables.  The best model of those attempted is random forest with a testing proportion of variance explained of $R^2$ = 0.81 and testing MSE = $8.3 \times 10^{10}$, a significant improvement on all regression models. Also, note that these are not true $R^2$ values as one would normally get when performing OLS regression but simply the proportion of variance explained by the model given the testing data. For this reason, MSE may be the better unbiased metric for comparison. 

```{r reg_table, results="asis"}

t.reg <- data.frame(rbind(c("BIC Selection", round(r.2.lm, 2), formatC(mse.lm, format="e", digits=2 )),
               c("Lasso", round(r.2.lasso_1,2), formatC(mse_lasso_1, format="e", digits=2 )),
               c("Lasso: Normalized", round(r.2.lasso_2,2), formatC(mse_lasso, format="e", digits=2 )),
               c("Random Forest", round(r.2.RF,2), formatC(mse.RF), format="e", digits=2 ))[,1:3])

names(t.reg) <- c("Method", "R^2", "MSE")
xkabledply(t.reg, title=NULL)
```



## Accuracy for Classifiers

Project one does not consider classification models; so, no comparisons will be attempted between the projects. Here, we can see that when only considering overall accuracy, the random forest again outperformed the other attempted models. 

```{r class_table, results="asis"}

t.cat <- data.frame(rbind(c("Tree", round(acc.tree, 3)),
               c("Random Forest", round(acc.RF, 3)),
               c("13-NN", 0.734)))

names(t.cat) <- c("Method", "Accuracy")
xkabledply(t.cat, title=NULL)
```


# Conclusion
By using these different modelling techniques, we have seen an improvement in our results compared to the linear regression model we built in project 1. Our classification models performed much better than our continuous methods overall. However, random forest outperformed all the other models in terms of accuracy. This aligns with previous research comparing modelling techniques for housing price prediction. The random forest classifier was the best out of the random forest methods we tried. We conclude that the random forest is the best modelling technique for predicting housing price using the Melbourne Housing Snapshot dataset.  


# Bibliography

Deng, Houtao & Runger, George & Tuv, Eugene. (2011). Bias of Importance Measures for Multi-valued Attributes and Solutions. Lecture Notes in Computer Science. 6792. 293-300. 10.1007/978-3-642-21738-8_38.

Mohd, T., Jamil, N. S., Johari, N., Abdullah, L., & Masrom, S. (2020). An overview of real estate modelling techniques for house price prediction. Charting a Sustainable Future of ASEAN in Business and Social Sciences, 321-338. doi:10.1007/978-981-15-3859-9_28

```{r close_stats_libs, include=F}
# unloadPkg(leaps)
# unloadPkg(ggplot2)
# unloadPkg(gridExtra)
# unloadPkg(scales)
# unloadPkg(heatmaply)
# unloadPkg(Hmisc)
# unloadPkg(faraway)
# unloadPkg(jtools)
```
