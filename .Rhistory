cutoff.high <- Sales.mean + Sales.sd
cutoff.low <- Sales.mean - Sales.sd
cutoff.low
cutoff.high
# create cuts
Carseats$Sales.cut[Carseats$Sales <= cutoff.low] <- "Low"
Carseats$Sales.cut[Carseats$Sales >= cutoff.high] <- "High"
Carseats$Sales.cut[cutoff.low < Carseats$Sales & Carseats$Sales < cutoff.high] <- "Medium"
# 70% training
train<- sample(nrow(Carseats), nrow(Carseats)*0.7)
#PS 1
# forest
mod.rf <- randomForest(Sales~., data=subset(Carseats, select=-Sales.cut), subset=train, mtry=10)
mod.rf
Carseats <- Carseats
Carseats <- Carseats
train<- sample(nrow(Carseats), nrow(Carseats)*0.7)
mod.rf <- randomForest(Sales~., data=subset(Carseats, select=-Sales.cut), subset=train, mtry=10)
mod.rf <- randomForest(Sales~., data=Carseats, subset=train, mtry=10)
yhat.rf <- predict(mod.rf, newdata=Carsears[-train, ] )
yhat.rf <- predict(mod.rf, newdata=Carseats[-train, ] )
y.test <- Carseats[-train, Sales]
y.test <- Carseats[-train, "Sales"]
y.test
plot(yhat.rf, y.test)
abline(0,1)
mean(( yhat.rf - y.test)^2)
importance(mod.rf)
mod.rf <- randomForest(Sales~., data=Carseats, subset=train, mtry=4, importance=T)
mod.rf
yhat.rf <- predict(mod.rf, newdata=Carseats[-train, ] )
plot(yhat.rf, y.test)
abline(0,1)
mse.rf <- mean(( yhat.rf - y.test)^2)
mse.rf
plot(mod.rf)
mod.rf <- randomForest(Sales~., data=Carseats, subset=train, mtry=5, importance=T)
mod.rf
yhat.rf <- predict(mod.rf, newdata=Carseats[-train, ] )
plot(yhat.rf, y.test)
abline(0,1)
mse.rf <- mean(( yhat.rf - y.test)^2)
mod.rf <- randomForest(Sales~., data=Carseats, subset=train, mtry=3, importance=T)
mod.rf
yhat.rf <- predict(mod.rf, newdata=Carseats[-train, ] )
plot(yhat.rf, y.test)
abline(0,1)
mse.rf <- mean(( yhat.rf - y.test)^2)
mod.bag <- randomForest(Sales~., data=Carseats, subset=train, mtry=10, importance=T)
mod.bag
yhat.bag <- predict(mod.bag, newdata=Carseats[-train, ] )
plot(yhat.bag, y.test)
abline(0,1)
mse.bag <- mean(( yhat.rf - y.test)^2)
mse.bag <- mean(( yhat.bag - y.test)^2)
mod.bag
mod.rf
mod.bag <- randomForest(Sales~., data=Carseats, subset=train, mtry=11, importance=T)
mod.bag <- randomForest(Sales~., data=Carseats, subset=train, mtry=10, importance=T, ntree=100)
mod.bag
yhat.bag <- predict(mod.bag, newdata=Carseats[-train, ] )
plot(yhat.bag, y.test)
abline(0,1)
mse.bag <- mean(( yhat.bag - y.test)^2)
?gbm
mod.gbm <- gbm(Sales~., data=Carseats, subset=train)
mod.gbm <- gbm(Sales~., data=Carseats[train,])
mod.gbm <- gbm(Sales~., data=Carseats[train,], distribution="gaussian", ntrees=5000, interaction.depth=4)
mod.gbm <- gbm(Sales~., data=Carseats[train,], distribution="gaussian", n.trees=500, interaction.depth=4)
summary(mod.gbm)
plot(importance(mod.bag))
yhat.gbm <- predict(mod.gbm, newdata=Carseats[-train, ] )
mse.gbm <- mean(( yhat.gbm - y.test)^2)
data.train <- Carseats[train, ]
data.test <- Carseats[-(train), ]
matrix.train <- xgb.DMatrix(data = as.matrix(data.train[!names(data.train) %in% c("Sales")]), label = data.train$Sales)
as.matrix(data.train[!names(data.train) %in% c("Sales")])
matrix.train <- xgb.DMatrix(data = as.matrix(data.train[!names(data.train) %in% c("Sales")]))
matrix.train <- xgb.DMatrix(data = data.train, label = data.train$Sales)
?xgb.DMatrix
?xgboost
mod.xgb <- xgboost(data=Carseats[train, ])
mod.xgb <- xgboost(data=as.matrix(Carseats[train, ]))
mod.xgb <- xgboost(data=as.matrix(Carseats[train, ]), label=Sales)
mod.xgb <- xgboost(data=as.matrix(Carseats[train, ]), label=Carseats[train, ]Sales)
mod.xgb <- train(Sales~., data=Carseats[train,])
factors <- c("Shelveloc", "Urban", "US")
Carseats[factors] <- factors(Carseats[factors])
Carseats[factors] <- factor(Carseats[factors])
Carseats[Shelveloc] <- factor(Carseats[Shelveloc])
Carseats[Urban] <- factor(Carseats[Urban])
Carseats[US] <- factor(Carseats[US])
Carseats[Shelveloc] <- factor(Carseats$Shelveloc)
Carseats[Urban] <- factor(Carseats$Urban)
Carseats[US] <- factor(Carseats$US)
Carseats[Shelveloc] <- factor(Carseats$Shelveloc)
Carseats$Shelveloc <- factor(Carseats$Shelveloc)
Carseats$Urban <- factor(Carseats$Urban)
Carseats$US <- factor(Carseats$US)
Carseats$Shelveloc
View(Carseats)
Carseats$ShelveLoc <- factor(Carseats$ShelveLoc)
View(Carseats)
Carseats$ShelveLoc <- factor(Carseats$ShelveLoc)
Carseats$Urban <- factor(Carseats$Urban)
Carseats$US <- factor(Carseats$US)
data.train <- Carseats[train, ]
data.test <- Carseats[-(train), ]
matrix.train <- xgb.DMatrix(data = as.matrix(data.train[!names(data.train) %in% c("Sales")]), label=data.train$Sales)
facotrs
factors
data.train <- Carseats[train, -("ShelveLoc", "Urban", "US")]
matrix.train <- xgb.DMatrix(data = as.matrix(data.train[!names(data.train) %in% c("Sales")]), label=data.train$Sales)
data.train
data.train <- Carseats[train, -c("ShelveLoc", "Urban", "US")]
data.train <- Carseats[train, -c(ShelveLoc, Urban, US)]
data.train <- subset(Carseats[train, ], select=-c(ShelveLoc, Urban, US))
data.train <- subset(Carseats[train, ], select=-c(ShelveLoc, Urban, US))
data.test <- subset(Carseats[-(train), ], select=-c(ShelveLoc, Urban, US))
matrix.train <- xgb.DMatrix(data = as.matrix(data.train[!names(data.train) %in% c("Sales")]), label=data.train$Sales)
mod.xgb <- xgboost(data=matrix.train, max_depth=3, eta = 0.2, nthread=3, nrounds=40, lambda=0
, objective="reg:linear")
matrix.test <- xgb.DMatrix(data = as.matrix(data.test[!names(data.test) %in% c("Sales")]), label=data.test$Sales)
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - boston.test)^2)
mse.xgb <- mean((yhat.xgb - y.test)^2)
rounds <- 5:50
rounds <- 5:50
train.mse <- rep(NA, 50)
for (r in rounds){
param <- list(objective = "reg:linear",
max_depth = 4,
eta = 0.2,
)
cv <- xgb.cv(data=matrix.train, params=param, nfold=5, nrounds=r)
}
rounds <- 5:50
train.mse <- rep(NA, 50)
for (r in rounds){
param <- list(objective = "reg:linear",
max_depth = 4,
eta = 0.2)
cv <- xgb.cv(data=matrix.train, params=param, nfold=5, nrounds=r)
}
View(cv)
cv$evaluation_log$test_rmse_mean
which.min(cv$evaluation_log$test_rmse_mean)
which.min(cv$evaluation_log$test_rmse_mean)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.2, nthread=3, nrounds=nrounds, lambda=0
, objective="reg:linear")
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.2, nthread=3, nrounds=nrounds, lambda=0
, objective="reg:linear")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
?xgb
cv <- xgb.cv(data=matrix.train, params=param, nfold=5, nrounds=r)
cv <- xgb.cv(data=matrix.train, objective = "reg:squarederror", max_depth=4, eta=0.2, nfold=5, nrounds=500, early_stopping_rounds=100)
View(cv)
?xgboost
cv <- xgb.cv(data=matrix.train, objective = "reg:squarederror", max_depth=4, eta=0.02, nfold=5, nrounds=500, early_stopping_rounds=100)
View(cv)
which.min(cv$evaluation_log$test_rmse_mean)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.02, nthread=3, nrounds=nrounds, lambda=0, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
cv <- xgb.cv(data=matrix.train, objective = "reg:squarederror", max_depth=4, eta=0.001, nfold=5, nrounds=5000, early_stopping_rounds=100)
which.min(cv$evaluation_log$test_rmse_mean)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.001, nthread=3, nrounds=nrounds, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
cv <- xgb.cv(data=matrix.train, objective = "reg:squarederror", max_depth=5, eta=0.001, nfold=5, nrounds=5000, early_stopping_rounds=100)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
cv <- xgb.cv(data=matrix.train, objective = "reg:squarederror", max_depth=5, eta=0.01, nfold=5, nrounds=5000, early_stopping_rounds=100)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
nrounds
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.001, nthread=3, nrounds=nrounds, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
cv <- xgb.cv(data=matrix.train, objective = "reg:squarederror", max_depth=4, eta=0.01, nfold=5, nrounds=5000, early_stopping_rounds=100)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.001, nthread=3, nrounds=nrounds, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.001, nrounds=nrounds, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.01, nrounds=nrounds, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
107.855 - 71.914
-7.0807 + 0 +0.7422Â 
logit(p)
-6.3385/(1-6.3385)
t=exp(-6.3385)
t/(1-t)
library(ISLR)
summary(Weekly)
boxplot(Weekly$Direction)
boxplot(factor(Weekly$Direction))
w<-Weekly
View(w)
plot(Weekly$Volume)
plot(Weekly$Today)
plot(Weekly$Direction)
plot(Year, Volume, data=Weekly)
plot(Weekly$Year, Weekly$Volume)
plot(Weekly$Year, Weekly$Today)
pairs(Weekly)
cor(Weekly[1:8])
Weekly.corr <- cor(Weekly[1:8])
heatmap(Weekly.corr)
heatmap(Weekly.corr, keep.dendro=F)
heatmap(Weekly.corr, keep.dendro=F)
stats::heatmap(Weekly.corr, keep.dendro=F)
stats::heatmap(Weekly.corr, keep.dendro=F)
stats::heatmap(Weekly.corr, Rowv="NA")
stats::heatmap(Weekly.corr, Rowv=NA)
heatmap(Weekly.corr, Rowv=NA, colv=NA)
library(ISLR)
pairs(Weekly)
plot(Weekly$Year, Weekly$Volume)
plot(Weekly$Year, Weekly$Today)
Weekly.corr <- cor(Weekly[1:8])
heatmap(Weekly.corr, Rowv=NA, Colv=NA)
Weekly.corr
View(Weekly.corr)
library(heatmaply)
heatmaply(Weekly.corr, Rowv=NA, Colv=NA)
heatmaply(Weekly.corr, dendrogram = F)
Weekly.corr <- cor(Weekly[1:8], method="Spearman")
Weekly.corr <- cor(Weekly[1:8], method="spearman")
heatmaply(Weekly.corr, dendrogram = F)
Weekly.corr
heatmaply(Weekly.corr, dendrogram = F
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="black",high = "yellow", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="black",high = "yellow", midpoint = 0,
limits = c(-1, 1))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="black",high = "yellow", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="black",high = "yellow", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="white",high = "yellow", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "blue", mid="white",high = "red", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "blue", mid="white",high = "red", midpoint = 0,
limits = c(-1, 1)),
grid_color="black")
plot(Weekly$Year, Weekly$Volume)
plot(Weekly$Year, Weekly$Today)
summary(Weekly)
mod.b <-  glm(Direction~ lag1 + lag2 + lag3 + lag4 + lag5, data=Weekly, family=binomial)
mod.b <-  glm(Direction~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data=Weekly, family=binomial)
summary(mod.b)
library(caret)
install.packages("caret")
library(caret)
View(mod.b)
#10.c
confusionMatrix(data=)
mod.b$fitted.values > 0.5
pred <-  mod.b$fitted.values > 0.5
if(mod.b$fitted.values > 0.5){pred.b<-"Up"} else{pred.b<-"Down"}
pred.b <- mod.b$fitted.values>0.5
table(pred.b, Weekly$Direction)
pred.b <- mod.b$fitted.values>0.7
table(pred.b, Weekly$Direction)
pred.b <- mod.b$fitted.values>0.6
table(pred.b, Weekly$Direction)
pred.b <- mod.b$fitted.values>0.55
table(pred.b, Weekly$Direction)
library(pROC)
564+49
363+232
pred.b <- mod.b$fitted.values>0.5
table(pred.b, Weekly$Direction)
613/1089
564+41
565/605
49+435
49/484
mod.d <-  glm(Direction~ Lag2, data=Weekly, family=binomial)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, Weekly$Direction)
#10.d (using 0.5 cutoff)
mod.d <-  glm(Direction~ Lag2, data=subset(Weekly, Year<2009), family=binomial)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, Weekly$Direction)
mod.d <-  glm(Direction~ Lag2, data=subset(Weekly, select=Year<2009), family=binomial)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, Weekly$Direction)
summary(subset(Weekly, select=Year<2009))
?subset
mod.d <-  glm(Direction~ Lag2, data=subset(Weekly, subset=Year<2009), family=binomial)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, Weekly$Direction)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, subset(Weekly, subset=Year<2009)$Direction)
summary(mod.b$fitted.values)
summary(predict(mod.b, Weekly, type="response"))
summary(subset(Weekly, subset=Year<2009))
pred.d <- predict(mod.d, subset(Weekly, subset=Year>=2009), type="response")
table(pred.d, subset(Weekly, subset=Year>=2009)$Direction)
pred.d <- predict(mod.d, subset(Weekly, subset=Year>=2009), type="response") > 0.5
table(pred.d, subset(Weekly, subset=Year>=2009)$Direction)
64+39
64/103
56/61
9/43
library(MASS)
mod.e <- lda(Direction~Lag2, data=data=subset(Weekly, subset=Year<2009), family=binomial)
?lda
mod.e <- lda(Direction~Lag2, data=subset(Weekly, subset=Year<2009), family=binomial)
pred.e <- predict(mod.e, subset(Weekly, subset=Year>=2009), type="response") > 0.5
pred.e <- predict(mod.e, subset(Weekly, subset=Year>=2009)) > 0.5
predict(mod.e, subset(Weekly, subset=Year>=2009))
predict(mod.e, subset(Weekly, subset=Year>=2009))$class
pred.e <- predict(mod.e, subset(Weekly, subset=Year>=2009))$class
table(pred.e, subset(Weekly, subset=Year>=2009)$Direction)
table(pred.d, subset(Weekly, subset=Year>=2009)$Direction)
# Robert Long
# HW 7
library(ISLR)
library(heatmaply)
library(MASS)
# 10.a
pairs(Weekly)
plot(Weekly$Year, Weekly$Volume)
plot(Weekly$Year, Weekly$Today)
Weekly.corr <- cor(Weekly[1:8], method="spearman")
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "blue", mid="white",high = "red", midpoint = 0,
limits = c(-1, 1)),
grid_color="black")
summary(Weekly)
#10.b
mod.b <-  glm(Direction~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data=Weekly, family=binomial)
summary(mod.b)
#10.c (using 0.5 cutoff)
pred.b <- mod.b$fitted.values>0.5
table(pred.b, Weekly$Direction)
#10.d (using 0.5 cutoff)
mod.d <-  glm(Direction~ Lag2, data=subset(Weekly, subset=Year<2009), family=binomial)
pred.d <- predict(mod.d, subset(Weekly, subset=Year>=2009), type="response") > 0.5
table(pred.d, subset(Weekly, subset=Year>=2009)$Direction)
# 10.e
mod.e <- lda(Direction~Lag2, data=subset(Weekly, subset=Year<2009), family=binomial)
pred.e <- predict(mod.e, subset(Weekly, subset=Year>=2009))$class
table(pred.e, subset(Weekly, subset=Year>=2009)$Direction)
forest.reg <- function(train, test){
# factors < 53 cats
model <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
r.2 <- r2(p, test$Price)
return(r.2)
}
forest.cat <- function(train, test){
model <- randomForest(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
acc <- sum(p == test$Price.cuts, na.rm=T)/length(p)
return(acc)
}
forest.reg(train, test)
# loads the data for training set param=T otherwise, testing set
load_data <- function(train=T){
data.full <- read.csv("melb_data.csv")
# replace NA with mean for suburb
data.full <- interpolate(data.full)
# if suburb not possible, use column mean
data.full$YearBuilt[is.na(data.full$YearBuilt)] <- mean(data.full$YearBuilt, na.rm=TRUE)
data.full$BuildingArea[is.na(data.full$BuildingArea)] <- mean(data.full$BuildingArea, na.rm=TRUE)
# create categories
data.full$Price.cuts <- as.character(cut(data.full$Price, 10))
data.full$Price.cuts[data.full$Price > 3650000] <- "(3.65e+06,9.01e+06]"
# convert to factors
data.full[sapply(data.full, is.character)] <- lapply(data.full[sapply(data.full, is.character)], as.factor)
data.full$Postcode <- factor(data.full$Postcode)
# address is unique therefor not a helpful predictor
data.full <- subset(data.full, select=-c(Address))
# create train and test data
samps <- read.csv("sample.txt", sep=" ")
data.train <- data.full[(samps$x),]
data.test <- data.full[(-samps$x),]
if (train==T){
return(data.train)
}
else{
return(data.test)
}
}
interpolate <- function(data){
for (i in 1:nrow(data)){
if (is.na(data[i, "Car"])){
suburb <- data[i, "Suburb"]
data[i, "Car"] <- mean(subset(data, Suburb==suburb)$Car, na.rm=T)
}
if (is.na(data[i, "BuildingArea"])){
suburb <- data[i, "Suburb"]
data[i, "BuildingArea"] <- mean(subset(data, Suburb==suburb)$BuildingArea, na.rm=T)
}
if (is.na(data[i, "YearBuilt"])){
suburb <- data[i, "Suburb"]
data[i, "YearBuilt"] <- mean(subset(data, Suburb==suburb)$YearBuilt, na.rm=T)
}
}
return(data)
}
train<-load_data()
setwd("~/Documents/GitHub/DATS6101_proj1_team2_S2021")
# loads the data for training set param=T otherwise, testing set
load_data <- function(train=T){
data.full <- read.csv("melb_data.csv")
# replace NA with mean for suburb
data.full <- interpolate(data.full)
# if suburb not possible, use column mean
data.full$YearBuilt[is.na(data.full$YearBuilt)] <- mean(data.full$YearBuilt, na.rm=TRUE)
data.full$BuildingArea[is.na(data.full$BuildingArea)] <- mean(data.full$BuildingArea, na.rm=TRUE)
# create categories
data.full$Price.cuts <- as.character(cut(data.full$Price, 10))
data.full$Price.cuts[data.full$Price > 3650000] <- "(3.65e+06,9.01e+06]"
# convert to factors
data.full[sapply(data.full, is.character)] <- lapply(data.full[sapply(data.full, is.character)], as.factor)
data.full$Postcode <- factor(data.full$Postcode)
# address is unique therefor not a helpful predictor
data.full <- subset(data.full, select=-c(Address))
# create train and test data
samps <- read.csv("sample.txt", sep=" ")
data.train <- data.full[(samps$x),]
data.test <- data.full[(-samps$x),]
if (train==T){
return(data.train)
}
else{
return(data.test)
}
}
interpolate <- function(data){
for (i in 1:nrow(data)){
if (is.na(data[i, "Car"])){
suburb <- data[i, "Suburb"]
data[i, "Car"] <- mean(subset(data, Suburb==suburb)$Car, na.rm=T)
}
if (is.na(data[i, "BuildingArea"])){
suburb <- data[i, "Suburb"]
data[i, "BuildingArea"] <- mean(subset(data, Suburb==suburb)$BuildingArea, na.rm=T)
}
if (is.na(data[i, "YearBuilt"])){
suburb <- data[i, "Suburb"]
data[i, "YearBuilt"] <- mean(subset(data, Suburb==suburb)$YearBuilt, na.rm=T)
}
}
return(data)
}
train<-load_data()
test<-load_data(F)
r2 <- function(y.predict, y.actual=y.test){
TSS <- sum((y.actual - mean(y.actual))^2, na.rm=T)
RSS <- sum((y.predict - y.actual)^2, na.rm=T)
rSq <- 1 - RSS/TSS
return(rSq)
}
trees <- function(train, test){
# factors < 32 cats
model <-tree(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)), na.action=na.roughfix)
p <- predict(model, test, type="class")
acc <- sum(p == test$Price.cuts)/length(p)
return(acc)
}
trees(train, test)
library(tree)
trees(train, test)
# factors < 32 cats
model <-tree(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)), na.action=na.roughfix)
p <- predict(model, test, type="class")
acc <- sum(p == test$Price.cuts)/length(p)
model <-tree(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)), na.action=na.pass)
p <- predict(model, test, type="class")
acc <- sum(p == test$Price.cuts)/length(p)
forest.reg <- function(train, test){
# factors < 53 cats
model <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
r.2 <- r2(p, test$Price)
return(r.2)
}
library(randomForest)
model <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
r.2 <- r2(p, test$Price)
model <- randomForest(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
acc <- sum(p == test$Price.cuts, na.rm=T)/length(p)
