cv <- xgb.cv(data=matrix.train, objective = "reg:squarederror", max_depth=4, eta=0.01, nfold=5, nrounds=5000, early_stopping_rounds=100)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.001, nthread=3, nrounds=nrounds, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
nrounds <- which.min(cv$evaluation_log$test_rmse_mean)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.001, nrounds=nrounds, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
mod.xgb <- xgboost(data=matrix.train, max_depth=4, eta = 0.01, nrounds=nrounds, objective="reg:squarederror")
yhat.xgb <- predict(mod.xgb, matrix.test)
mse.xgb <- mean((yhat.xgb - y.test)^2)
107.855 - 71.914
-7.0807 + 0 +0.7422 
logit(p)
-6.3385/(1-6.3385)
t=exp(-6.3385)
t/(1-t)
library(ISLR)
summary(Weekly)
boxplot(Weekly$Direction)
boxplot(factor(Weekly$Direction))
w<-Weekly
View(w)
plot(Weekly$Volume)
plot(Weekly$Today)
plot(Weekly$Direction)
plot(Year, Volume, data=Weekly)
plot(Weekly$Year, Weekly$Volume)
plot(Weekly$Year, Weekly$Today)
pairs(Weekly)
cor(Weekly[1:8])
Weekly.corr <- cor(Weekly[1:8])
heatmap(Weekly.corr)
heatmap(Weekly.corr, keep.dendro=F)
heatmap(Weekly.corr, keep.dendro=F)
stats::heatmap(Weekly.corr, keep.dendro=F)
stats::heatmap(Weekly.corr, keep.dendro=F)
stats::heatmap(Weekly.corr, Rowv="NA")
stats::heatmap(Weekly.corr, Rowv=NA)
heatmap(Weekly.corr, Rowv=NA, colv=NA)
library(ISLR)
pairs(Weekly)
plot(Weekly$Year, Weekly$Volume)
plot(Weekly$Year, Weekly$Today)
Weekly.corr <- cor(Weekly[1:8])
heatmap(Weekly.corr, Rowv=NA, Colv=NA)
Weekly.corr
View(Weekly.corr)
library(heatmaply)
heatmaply(Weekly.corr, Rowv=NA, Colv=NA)
heatmaply(Weekly.corr, dendrogram = F)
Weekly.corr <- cor(Weekly[1:8], method="Spearman")
Weekly.corr <- cor(Weekly[1:8], method="spearman")
heatmaply(Weekly.corr, dendrogram = F)
Weekly.corr
heatmaply(Weekly.corr, dendrogram = F
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="black",high = "yellow", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="black",high = "yellow", midpoint = 0,
limits = c(-1, 1))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="black",high = "yellow", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="black",high = "yellow", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "magenta", mid="white",high = "yellow", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "blue", mid="white",high = "red", midpoint = 0,
limits = c(-1, 1)))
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "blue", mid="white",high = "red", midpoint = 0,
limits = c(-1, 1)),
grid_color="black")
plot(Weekly$Year, Weekly$Volume)
plot(Weekly$Year, Weekly$Today)
summary(Weekly)
mod.b <-  glm(Direction~ lag1 + lag2 + lag3 + lag4 + lag5, data=Weekly, family=binomial)
mod.b <-  glm(Direction~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data=Weekly, family=binomial)
summary(mod.b)
library(caret)
install.packages("caret")
library(caret)
View(mod.b)
#10.c
confusionMatrix(data=)
mod.b$fitted.values > 0.5
pred <-  mod.b$fitted.values > 0.5
if(mod.b$fitted.values > 0.5){pred.b<-"Up"} else{pred.b<-"Down"}
pred.b <- mod.b$fitted.values>0.5
table(pred.b, Weekly$Direction)
pred.b <- mod.b$fitted.values>0.7
table(pred.b, Weekly$Direction)
pred.b <- mod.b$fitted.values>0.6
table(pred.b, Weekly$Direction)
pred.b <- mod.b$fitted.values>0.55
table(pred.b, Weekly$Direction)
library(pROC)
564+49
363+232
pred.b <- mod.b$fitted.values>0.5
table(pred.b, Weekly$Direction)
613/1089
564+41
565/605
49+435
49/484
mod.d <-  glm(Direction~ Lag2, data=Weekly, family=binomial)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, Weekly$Direction)
#10.d (using 0.5 cutoff)
mod.d <-  glm(Direction~ Lag2, data=subset(Weekly, Year<2009), family=binomial)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, Weekly$Direction)
mod.d <-  glm(Direction~ Lag2, data=subset(Weekly, select=Year<2009), family=binomial)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, Weekly$Direction)
summary(subset(Weekly, select=Year<2009))
?subset
mod.d <-  glm(Direction~ Lag2, data=subset(Weekly, subset=Year<2009), family=binomial)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, Weekly$Direction)
pred.d <- mod.d$fitted.values>0.5
table(pred.d, subset(Weekly, subset=Year<2009)$Direction)
summary(mod.b$fitted.values)
summary(predict(mod.b, Weekly, type="response"))
summary(subset(Weekly, subset=Year<2009))
pred.d <- predict(mod.d, subset(Weekly, subset=Year>=2009), type="response")
table(pred.d, subset(Weekly, subset=Year>=2009)$Direction)
pred.d <- predict(mod.d, subset(Weekly, subset=Year>=2009), type="response") > 0.5
table(pred.d, subset(Weekly, subset=Year>=2009)$Direction)
64+39
64/103
56/61
9/43
library(MASS)
mod.e <- lda(Direction~Lag2, data=data=subset(Weekly, subset=Year<2009), family=binomial)
?lda
mod.e <- lda(Direction~Lag2, data=subset(Weekly, subset=Year<2009), family=binomial)
pred.e <- predict(mod.e, subset(Weekly, subset=Year>=2009), type="response") > 0.5
pred.e <- predict(mod.e, subset(Weekly, subset=Year>=2009)) > 0.5
predict(mod.e, subset(Weekly, subset=Year>=2009))
predict(mod.e, subset(Weekly, subset=Year>=2009))$class
pred.e <- predict(mod.e, subset(Weekly, subset=Year>=2009))$class
table(pred.e, subset(Weekly, subset=Year>=2009)$Direction)
table(pred.d, subset(Weekly, subset=Year>=2009)$Direction)
# Robert Long
# HW 7
library(ISLR)
library(heatmaply)
library(MASS)
# 10.a
pairs(Weekly)
plot(Weekly$Year, Weekly$Volume)
plot(Weekly$Year, Weekly$Today)
Weekly.corr <- cor(Weekly[1:8], method="spearman")
heatmaply(Weekly.corr, dendrogram = F,
scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
low = "blue", mid="white",high = "red", midpoint = 0,
limits = c(-1, 1)),
grid_color="black")
summary(Weekly)
#10.b
mod.b <-  glm(Direction~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data=Weekly, family=binomial)
summary(mod.b)
#10.c (using 0.5 cutoff)
pred.b <- mod.b$fitted.values>0.5
table(pred.b, Weekly$Direction)
#10.d (using 0.5 cutoff)
mod.d <-  glm(Direction~ Lag2, data=subset(Weekly, subset=Year<2009), family=binomial)
pred.d <- predict(mod.d, subset(Weekly, subset=Year>=2009), type="response") > 0.5
table(pred.d, subset(Weekly, subset=Year>=2009)$Direction)
# 10.e
mod.e <- lda(Direction~Lag2, data=subset(Weekly, subset=Year<2009), family=binomial)
pred.e <- predict(mod.e, subset(Weekly, subset=Year>=2009))$class
table(pred.e, subset(Weekly, subset=Year>=2009)$Direction)
forest.reg <- function(train, test){
# factors < 53 cats
model <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
r.2 <- r2(p, test$Price)
return(r.2)
}
forest.cat <- function(train, test){
model <- randomForest(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
acc <- sum(p == test$Price.cuts, na.rm=T)/length(p)
return(acc)
}
forest.reg(train, test)
# loads the data for training set param=T otherwise, testing set
load_data <- function(train=T){
data.full <- read.csv("melb_data.csv")
# replace NA with mean for suburb
data.full <- interpolate(data.full)
# if suburb not possible, use column mean
data.full$YearBuilt[is.na(data.full$YearBuilt)] <- mean(data.full$YearBuilt, na.rm=TRUE)
data.full$BuildingArea[is.na(data.full$BuildingArea)] <- mean(data.full$BuildingArea, na.rm=TRUE)
# create categories
data.full$Price.cuts <- as.character(cut(data.full$Price, 10))
data.full$Price.cuts[data.full$Price > 3650000] <- "(3.65e+06,9.01e+06]"
# convert to factors
data.full[sapply(data.full, is.character)] <- lapply(data.full[sapply(data.full, is.character)], as.factor)
data.full$Postcode <- factor(data.full$Postcode)
# address is unique therefor not a helpful predictor
data.full <- subset(data.full, select=-c(Address))
# create train and test data
samps <- read.csv("sample.txt", sep=" ")
data.train <- data.full[(samps$x),]
data.test <- data.full[(-samps$x),]
if (train==T){
return(data.train)
}
else{
return(data.test)
}
}
interpolate <- function(data){
for (i in 1:nrow(data)){
if (is.na(data[i, "Car"])){
suburb <- data[i, "Suburb"]
data[i, "Car"] <- mean(subset(data, Suburb==suburb)$Car, na.rm=T)
}
if (is.na(data[i, "BuildingArea"])){
suburb <- data[i, "Suburb"]
data[i, "BuildingArea"] <- mean(subset(data, Suburb==suburb)$BuildingArea, na.rm=T)
}
if (is.na(data[i, "YearBuilt"])){
suburb <- data[i, "Suburb"]
data[i, "YearBuilt"] <- mean(subset(data, Suburb==suburb)$YearBuilt, na.rm=T)
}
}
return(data)
}
train<-load_data()
setwd("~/Documents/GitHub/DATS6101_proj1_team2_S2021")
# loads the data for training set param=T otherwise, testing set
load_data <- function(train=T){
data.full <- read.csv("melb_data.csv")
# replace NA with mean for suburb
data.full <- interpolate(data.full)
# if suburb not possible, use column mean
data.full$YearBuilt[is.na(data.full$YearBuilt)] <- mean(data.full$YearBuilt, na.rm=TRUE)
data.full$BuildingArea[is.na(data.full$BuildingArea)] <- mean(data.full$BuildingArea, na.rm=TRUE)
# create categories
data.full$Price.cuts <- as.character(cut(data.full$Price, 10))
data.full$Price.cuts[data.full$Price > 3650000] <- "(3.65e+06,9.01e+06]"
# convert to factors
data.full[sapply(data.full, is.character)] <- lapply(data.full[sapply(data.full, is.character)], as.factor)
data.full$Postcode <- factor(data.full$Postcode)
# address is unique therefor not a helpful predictor
data.full <- subset(data.full, select=-c(Address))
# create train and test data
samps <- read.csv("sample.txt", sep=" ")
data.train <- data.full[(samps$x),]
data.test <- data.full[(-samps$x),]
if (train==T){
return(data.train)
}
else{
return(data.test)
}
}
interpolate <- function(data){
for (i in 1:nrow(data)){
if (is.na(data[i, "Car"])){
suburb <- data[i, "Suburb"]
data[i, "Car"] <- mean(subset(data, Suburb==suburb)$Car, na.rm=T)
}
if (is.na(data[i, "BuildingArea"])){
suburb <- data[i, "Suburb"]
data[i, "BuildingArea"] <- mean(subset(data, Suburb==suburb)$BuildingArea, na.rm=T)
}
if (is.na(data[i, "YearBuilt"])){
suburb <- data[i, "Suburb"]
data[i, "YearBuilt"] <- mean(subset(data, Suburb==suburb)$YearBuilt, na.rm=T)
}
}
return(data)
}
train<-load_data()
test<-load_data(F)
r2 <- function(y.predict, y.actual=y.test){
TSS <- sum((y.actual - mean(y.actual))^2, na.rm=T)
RSS <- sum((y.predict - y.actual)^2, na.rm=T)
rSq <- 1 - RSS/TSS
return(rSq)
}
trees <- function(train, test){
# factors < 32 cats
model <-tree(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)), na.action=na.roughfix)
p <- predict(model, test, type="class")
acc <- sum(p == test$Price.cuts)/length(p)
return(acc)
}
trees(train, test)
library(tree)
trees(train, test)
# factors < 32 cats
model <-tree(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)), na.action=na.roughfix)
p <- predict(model, test, type="class")
acc <- sum(p == test$Price.cuts)/length(p)
model <-tree(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)), na.action=na.pass)
p <- predict(model, test, type="class")
acc <- sum(p == test$Price.cuts)/length(p)
forest.reg <- function(train, test){
# factors < 53 cats
model <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
r.2 <- r2(p, test$Price)
return(r.2)
}
library(randomForest)
model <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
r.2 <- r2(p, test$Price)
model <- randomForest(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
acc <- sum(p == test$Price.cuts, na.rm=T)/length(p)
source('~/Documents/GitHub/DATS6101_proj1_team2_S2021/P2_support.R')
# loads the data for training set param=T otherwise, testing set
load_data <- function(train=T){
data.full <- read.csv("melb_data.csv")
# replace NA with mean for suburb
data.full <- interpolate(data.full)
# if suburb not possible, use column mean
data.full$YearBuilt[is.na(data.full$YearBuilt)] <- mean(data.full$YearBuilt, na.rm=TRUE)
data.full$BuildingArea[is.na(data.full$BuildingArea)] <- mean(data.full$BuildingArea, na.rm=TRUE)
# create categories
data.full$Price.cuts <- as.character(cut(data.full$Price, 10))
data.full$Price.cuts[data.full$Price > 7.61e+04 & data.full$Price <= 9.76e+05] <- 1
data.full$Price.cuts[data.full$Price > 9.76e+05 & data.full$Price <= 1.87e+06] <- 2
data.full$Price.cuts[data.full$Price > 1.87e+06 & data.full$Price <= 2.76e+06] <- 3
data.full$Price.cuts[data.full$Price > 2.76e+06 & data.full$Price <= 3.65e+06] <- 4
data.full$Price.cuts[data.full$Price > 3.65e+06] <- 5
# convert to factors
data.full[sapply(data.full, is.character)] <- lapply(data.full[sapply(data.full, is.character)], as.factor)
data.full$Postcode <- factor(data.full$Postcode)
# address is unique therefor not a helpful predictor
data.full <- subset(data.full, select=-c(Address))
# create train and test data
samps <- read.csv("sample.txt", sep=" ")
data.train <- data.full[(samps$x),]
data.test <- data.full[(-samps$x),]
if (train==T){
return(data.train)
}
else{
return(data.test)
}
}
interpolate <- function(data){
for (i in 1:nrow(data)){
if (is.na(data[i, "Car"])){
suburb <- data[i, "Suburb"]
data[i, "Car"] <- mean(subset(data, Suburb==suburb)$Car, na.rm=T)
}
if (is.na(data[i, "BuildingArea"])){
suburb <- data[i, "Suburb"]
data[i, "BuildingArea"] <- mean(subset(data, Suburb==suburb)$BuildingArea, na.rm=T)
}
if (is.na(data[i, "YearBuilt"])){
suburb <- data[i, "Suburb"]
data[i, "YearBuilt"] <- mean(subset(data, Suburb==suburb)$YearBuilt, na.rm=T)
}
}
return(data)
}
r2 <- function(y.predict, y.actual=y.test){
TSS <- sum((y.actual - mean(y.actual))^2, na.rm=T)
RSS <- sum((y.predict - y.actual)^2, na.rm=T)
rSq <- 1 - RSS/TSS
return(rSq)
}
# Trees and Forests
trees <- function(train, test){
# factors < 32 cats
model <-tree(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)), na.action=na.pass)
p <- predict(model, test, type="class")
acc <- sum(p == test$Price.cuts)/length(p)
return(acc)
}
forest.reg <- function(train, test){
# factors < 53 cats
model <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
r.2 <- r2(p, test$Price)
return(r.2)
}
forest.cat <- function(train, test){
# factors < 53 cats
model <- randomForest(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test, na.action=na.roughfix)
acc <- sum(p == test$Price.cuts, na.rm=T)/length(p)
return(acc)
}
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
knitr::opts_chunk$set(echo = F)
options(scientific=T, digits = 5)
options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
source("general_support.R", local = knitr::knit_global())
source("P2_support.R", local = knitr::knit_global())
train <- load_data()
test <- load_data(F)
loadPkg("party")
?ctree
# factors < 53 cats
mod.tree <-ctree(factor(Price.cuts)~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)))
p <- predict(mod.tree, test, type="class")
p <- predict(mod.tree, test, type="response")
acc <- sum(p == test$Price.cuts)/length(p)
plot(c.tree)
plot(mod.tree)
mod.tree
loadPkg("tree")
# factors < 53 cats
mod.tree <-tree(factor(Price.cuts)~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)))
mod.tree <-tree(factor(Price.cuts)~., data=subset(train, select=-c(Price, SellerG, Date, Postcode)))
str(train)
mod.tree <-tree(factor(Price.cuts)~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)))
p <- predict(mod.tree, test, type="response")
p <- predict(mod.tree, test, type="class")
acc <- sum(p == test$Price.cuts)/length(p)
table(p)
1754+2320
table(p, test$Price.cuts)
table(test$Price.cuts)
loadPkg("caret")
confusionMatrix(test$Price.cuts, p)
?confusionMatrix
cm.tree <- confusionMatrix(test$Price.cuts, p)
View(cm.tree)
cm.tree$table
xkabledply(cm.tree$table)
xkablesummary(cm.tree$table)
table(p)
cm.tree <- confusionMatrix(p, test$Price.cuts)
xkablesummary(cm.tree$table)
cm.tree$table
View(cm.tree)
View(mod.tree)
mod.tree[["frame"]][["n"]]
mod.tree[["frame"]][["yval"]]
mod.tree[["frame"]][["splits"]]
acc
loadPkg("randomForest")
loadPkg("randomForest")
# factors < 53 cats
mod.RF <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test)
r.2 <- r2(p, test$Price)
loadPkg("randomForest")
# factors < 53 cats
mod.RF <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(model, test)
r.2 <- r2(p, test$Price)
p <- predict(model, test)
p <- predict(mod.RF , test)
r.2 <- r2(p, test$Price)
mean((p-test$Price)^2)
mod.RF.cat <- randomForest(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
p <- predict(mod.RF.cat, test, na.action=na.roughfix)
acc <- sum(p == test$Price.cuts, na.rm=T)/length(p)
cm.RF <- confusionMatrix(p, test$price)
cm.RF <- confusionMatrix(p, test$Price.cuts)
cm.RF$table
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
knitr::opts_chunk$set(echo = F)
options(scientific=T, digits = 5)
options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg("leaps")
loadPkg("ggplot2")
loadPkg("gridExtra")
loadPkg("scales")
loadPkg("heatmaply")
loadPkg("Hmisc")
loadPkg("faraway")
set.seed(17)
data.full <- read.csv("melb_data.csv")
# create train and test data
samps <- read.csv("sample.txt", sep=" ")
data.train <- data.full[(samps$x),]
data.test <- data.full[(-samps$x),]
numerics <- c("Rooms", "Price", "Distance", "Bedroom2", "Bathroom",
"Car", "Landsize", "BuildingArea", "YearBuilt",
"Lattitude", "Longtitude",
"Propertycount")
View(mod.RF)
?randomForest
