---
title: "Melbourne Housing Market - Presentation 2"
subtitle: "Intro Data Science"
author: "Team 2"
date: "`r Sys.Date()`"
output: ioslides_presentation
---



```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
knitr::opts_chunk$set(echo = F)
options(scientific=T, digits = 5) 
options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times

source("P2_support.R", local = knitr::knit_global())

```


```{r stats_libs, include=F}
loadPkg("leaps")
# loadPkg("ggplot2")
# loadPkg("gridExtra")
# loadPkg("scales")
# loadPkg("heatmaply")
# loadPkg("Hmisc")
# loadPkg("faraway")
# loadPkg("jtools")
loadPkg("caret")

```



```{r data, include=F}
# run this chunk to load data so that we all use same data for training/testing

train <- load_data()
test <- load_data(F)

```

## Dataset
- Home Sales in 2017
  - Location
  - Construction
  - Sale
- Variables: 20
  - Numeric: 12
  - Categorical: 8
  
## Goals
1.	Understand which attributes of a home and its sale determine final sale price
2.	Attempt to build a reasonable model for inference and/or prediction for final sale price


## Exploratory Data Analysis

```{r eda, results='markup'}
hist(test$Price)
```


## New for Project 2

Created 5 `Price` based categories

Imputed missing data based on suburb




## Linear Model Variable Selection
```{r }
loadPkg("dplyr")

# df<-data.frame(read.csv("melb_data.csv"))

df1 <- select(train,
            Price, Distance, Rooms, Bedroom2, Bathroom, Car, 
            Landsize, Lattitude, Longtitude, 
            Propertycount, BuildingArea, YearBuilt)

reg.best5 <- regsubsets(Price~. , data = df1, nvmax = 5, nbest = 1, method = "exhaustive")

plot(reg.best5, scale = "bic", main = "BIC")

mod.select <- lm(Price ~ Distance + Rooms + Lattitude + BuildingArea + YearBuilt, data=train)

p.lm <- predict(mod.select, test)

r.2.lm <- r2(p.lm, test$Price)

mse.lm <- mean((p.lm - test$Price)^2)
```

The best model is based on five variables: `Distance`, `Rooms`, `Lattitude`, `BuildingArea`, `YearBuilt` 





## Regulization: Lasso regression

```{r regularization process for data preparation, include= FALSE}
# data for trainning model
train.X <- data.matrix(train[,-which(names(train) %in% c("Price", "Price.cuts"))])
train.y <- data.matrix(log(train$Price))

# data for testing model
test.X <- data.matrix(test[,-which(names(test) %in% c("Price", "Price.cuts"))])
test.y <- data.matrix(log(test$Price))

```


```{r lasso, results='markup'}
loadPkg("glmnet")

lasso <- cv.glmnet(x = train.X, y = train.y, nfolds = 10)

plot(lasso)

```


## Coefficients

```{r coef in lasso regression}

coef(lasso, s = 'lambda.min', exact = TRUE)
min(lasso$cvm)

```

From the coefficient of lasso regression model, the variables of `Rooms`, `Type`, `Distance`, `Postcode`,  `Bathroom`, `YearBuilt`, `Lattitude`, and `Longtitude` are related with response variable `Price`.


## Lasso Results

```{r lasso fit, include=TRUE, results='markup'}
set.seed(999)
lasso_pred <- as.numeric(exp(predict(lasso, newx = test.X, s = "lambda.min"))-1)
hist(lasso_pred, main="Histogram of Lasso Predictions", xlab = "Predictions")

mse_lasso_1 = sum((lasso_pred-exp(test.y))^2)/length(test.y)

r.2.lasso_1 <- r2(lasso_pred, test$Price)
```

## Lasso Prediction

```{r lasso predict, include=TRUE, results='markup'}

plot(exp(test.y),lasso_pred,xlab="True  price",ylab="Predicted price",
     main="Prediction using Lasso regression")

text(-1,3,substitute(r^2 == r2,list(r2=cor(test.y,lasso_pred))),adj=0)
text(-1,2.7,substitute(MSE == r2,list(r2=mse_lasso_1)),adj=0)
abline(0,1)

```


## Lasso Results: Normalized

```{r predict with normalized price, include= TRUE, results='markup'}
lasso_fit = glmnet(train.X,train.y,alpha = 1,lambda = lasso$lambda.min)
pred.y = predict(lasso_fit,test.X)

mse_lasso = mean((exp(pred.y)-exp(test.y))^2)

plot(test.y,pred.y,xlab="True normalized price",ylab="Predicted normalized price",
     main="Prediction using Lasso regression")
text(-1,3,substitute(r^2 == r2,list(r2=cor(test.y,pred.y))),adj=0)
text(-1,2.7,substitute(MSE == r2,list(r2=mse_lasso)),adj=0)
abline(0,1)

r.2.lasso_2 <- r2(exp(pred.y), test$Price)
```




## Tree
```{r tree, results='markup'}
loadPkg("tree")


# factors < 32 cats
mod.tree <-tree(factor(Price.cuts)~., 
                data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)))
  
p <- predict(mod.tree, test, type="class")
  
acc.tree <- sum(p == test$Price.cuts)/length(p)

cm.tree <- confusionMatrix(p, test$Price.cuts)

cm.tree$table

```


Overall Accuracy: `r acc.tree`

Poor prediction at higher prices


## Random Forest: Regression

```{r RF_reg}
loadPkg("randomForest")

# factors < 53 cats
mod.RF <- randomForest(Price~., 
                       data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), 
                       importance=T, ntree=200)
  
p.RF_reg <- predict(mod.RF, test)
  
r.2.RF <- r2(p.RF_reg, test$Price)

mse.RF <- mean((p.RF_reg - test$Price)^2)
```

Trees: 200

Variables: 5

$R^2$ = `r r.2.RF`

MSE = `r mse.RF`


## Random Forest: Categorical

```{r RF_cats, results='markup'}
# factors < 53 cats
mod.RF.cat <- randomForest(Price.cuts~., 
                           data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), 
                           importance=T, ntree=200)
  
p <- predict(mod.RF.cat, test)

acc.RF <- sum(p == test$Price.cuts, na.rm=T)/length(p)

cm.RF <- confusionMatrix(p, test$Price.cuts)
cm.RF$table
```

Trees: 200

Variables: 5

Overall Accuracy: `r acc.RF`


<!-- ```{r data, include=F} -->
<!-- set.seed(17) -->

<!-- train <- load_data(T) -->
<!-- test <- load_data(F) -->

<!-- ``` -->

<!-- ```{r tree, include=FALSE} -->
<!-- train_tree <- trees(train=train, test=test) -->
<!-- ``` -->

<!-- ```{r randomForest, include= FALSE} -->
<!-- forest_reg_r2 <- forest.reg(train = train, test = test) -->
<!-- forest_cat_acc <- forest.cat(train = train, test = test) -->
<!-- ``` -->


<!-- ```{r data, include=F} -->
<!-- set.seed(17) -->

<!-- train <- load_data(T) -->
<!-- test <- load_data(F) -->

<!-- ``` -->








```{r close_stats_libs, include=F}
unloadPkg(leaps)
# unloadPkg(ggplot2)
# unloadPkg(gridExtra)
# unloadPkg(scales)
# unloadPkg(heatmaply)
# unloadPkg(Hmisc)
# unloadPkg(faraway)
# unloadPkg(jtools)
unloadPkg(tree)
unloadPkg(caret)
unloadPkg(randomForest)
unloadPkg(dplyr)
unloadPkg(glmnet)
```
