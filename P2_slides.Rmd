---
title: "Melbourne Housing Market - Presentation 2"
subtitle: "Intro Data Science"
author: "Team 2"
date: "`r Sys.Date()`"
output: ioslides_presentation
---



```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
knitr::opts_chunk$set(echo = F)
options(scientific=T, digits = 5) 
options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times

source("P2_support.R", local = knitr::knit_global())

```


```{r stats_libs, include=F}
# loadPkg("leaps")
# loadPkg("ggplot2")
# loadPkg("gridExtra")
# loadPkg("scales")
# loadPkg("heatmaply")
# loadPkg("Hmisc")
# loadPkg("faraway")
# loadPkg("jtools")
loadPkg("caret")
```



```{r data, include=F}

train <- load_data()
test <- load_data(F)

```

## Dataset
- Home Sales in 2017
  - Location
  - Construction
  - Sale
- Variables: 20
  - Numeric: 12
  - Categorical: 8
  
## Goals
1.	Understand which attributes of a home and its sale determine final sale price
2.	Attempt to build a reasonable model for inference and/or prediction for final sale price


## Exploratory Data Analysis

```{r eda, results='markup'}
hist(test$Price)
```

Created 5 `Price` based categories

## model selection
```{r}
library(dplyr)
df<-data.frame(read.csv("melb_data.csv"))
df1<-select(df,Price,Distance,Rooms,Bedroom2,Bathroom,Car,Landsize,Lattitude,Longtitude,Propertycount,BuildingArea,YearBuilt)
loadPkg("leaps")
reg.best5 <- regsubsets(Price~. , data = df1, nvmax = 5, nbest = 1, method = "exhaustive")
plot(reg.best10, scale = "bic", main = "BIC")

```
The best model is based on five variables:Distance, Rooms, Latitude, Buildingarea, Yearbuilt 


## Tree
```{r tree, results='markup'}
loadPkg("tree")


# factors < 32 cats
mod.tree <-tree(factor(Price.cuts)~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode, CouncilArea)))
  
p <- predict(mod.tree, test, type="class")
  
acc.tree <- sum(p == test$Price.cuts)/length(p)

cm.tree <- confusionMatrix(p, test$Price.cuts)

cm.tree$table

```


Overall Accuracy: `r acc.tree`

Poor prediction at higher prices


## Random Forest: Regression

```{r RF_reg}
loadPkg("randomForest")

# factors < 53 cats
mod.RF <- randomForest(Price~., data=subset(train, select=-c(Price.cuts, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
  
p <- predict(mod.RF , test)
  
r.2.RF <- r2(p, test$Price)
```

Trees: 200

Variables: 5

$R^2$ = `r r.2.RF`

MSE = `r mean((p-test$Price)^2)`


## Random Forest: Categorical

```{r RF_cats, results='markup'}
# factors < 53 cats
mod.RF.cat <- randomForest(Price.cuts~., data=subset(train, select=-c(Price, SellerG, Suburb, Date, Postcode)), importance=T, na.action=na.roughfix, ntree=200)
  
p <- predict(mod.RF.cat, test, na.action=na.roughfix)

acc.RF <- sum(p == test$Price.cuts, na.rm=T)/length(p)

cm.RF <- confusionMatrix(p, test$Price.cuts)
cm.RF$table
```

Trees: 200

Variables: 5

Overall Accuracy: `r acc.RF`










```{r data, include=F}
set.seed(17)

train <- load_data(T)
test <- load_data(F)

```

```{r tree, include=FALSE}
train_tree <- trees(train=train, test=test)
```

```{r randomForest, include= FALSE}
forest_reg_r2 <- forest.reg(train = train, test = test)
forest_cat_acc <- forest.cat(train = train, test = test)
```


```{r data, include=F}
set.seed(17)

train <- load_data(T)
test <- load_data(F)

```

## Regulization: Lasso regression

```{r regularization process for data preparation, include= FALSE}
# data for trainning model
train.X <- data.matrix(train[,-which(names(train) %in% c("Price", "Price.cuts"))])
train.y <- data.matrix(log(train$Price))

# data for testing model
test.X <- data.matrix(test[,-which(names(test) %in% c("Price", "Price.cuts"))])
test.y <- data.matrix(log(test$Price))

```

```{r lasso }
lasso <- cv.glmnet(x = train.X, y = train.y, nfolds = 10)
plot(lasso)

```


```{r coef in lasso regression }
coef(lasso, s = 'lambda.min', exact = TRUE)
min(lasso$cvm)

```

From the coefficient of lasso regression model, the variables of Rooms,Type, Distance, Postcode,  Bathroom,YearBuilt, Lattitude, Longtitude are related with responsive variabel Price.


```{r lasso fit and predict, include=TRUE}
set.seed(999)
lasso_pred <- as.numeric(exp(predict(lasso, newx = test.X, s = "lambda.min"))-1)
hist(lasso_pred, main="Histogram of Lasso Predictions", xlab = "Predictions")

mse_lasso_1 = sum((lasso_pred-exp(test.y))^2)/length(test.y)
plot(exp(test.y),lasso_pred,xlab="True  price",ylab="Predicted price",
     main="Prediction using Lasso regression")

text(-1,3,substitute(r^2 == r2,list(r2=cor(test.y,lasso_pred))),adj=0)
text(-1,2.7,substitute(MSE == r2,list(r2=mse_lasso_1)),adj=0)
abline(0,1)

```

```{r predict with normalized price, include= TRUE}
lasso_fit = glmnet(train.X,train.y,alpha = 1,lambda = lasso$lambda.min)
pred.y = predict(lasso_fit,test.X)

mse_lasso = sum((pred.y-test.y)^2)/length(test.y)

plot(test.y,pred.y,xlab="True normalized price",ylab="Predicted normalized price",
     main="Prediction using Lasso regression")
text(-1,3,substitute(r^2 == r2,list(r2=cor(test.y,pred.y))),adj=0)
text(-1,2.7,substitute(MSE == r2,list(r2=mse_lasso)),adj=0)
abline(0,1)
```


```{r close_stats_libs, include=F}
# unloadPkg(leaps)
# unloadPkg(ggplot2)
# unloadPkg(gridExtra)
# unloadPkg(scales)
# unloadPkg(heatmaply)
# unloadPkg(Hmisc)
# unloadPkg(faraway)
# unloadPkg(jtools)
unloadPkg(tree)
unloadPkg(caret)
unloadPkg(randomForest)
```
