---
title: "Melbourne Housing Market-Paper1"
subtitle: "Intro Data Science"
author: "Team 2"
date: "`r Sys.Date()`"
output:
   html_document:
     toc: true
     toc_float: true
     number_section: true
---




```{r basic, include=F}
# use this function to conveniently load libraries and work smoothly with knitting
# can add quietly=T option to the require() function
# the loadPkg function essentially replaced/substituted two functions install.packages() and library() in one step.
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }

# unload/detact package when done using it
unloadPkg = function(pkg, character.only = FALSE) { 
  if(!character.only) { pkg <- as.character(substitute(pkg)) } 
  search_item <- paste("package", pkg,sep = ":") 
  while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) } 
}
```


```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
knitr::opts_chunk$set(echo = F)
options(scientific=T, digits = 5) 
options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times

```

```{r xkablesummary}
loadPkg("xtable")
loadPkg("kableExtra")
loadPkg("stringi")

xkabledply = function(modelsmmrytable, title="Table", digits = 4, pos="left", bso="striped", wide=FALSE) { 
  #' Combining base::summary, xtable, and kableExtra, to easily display model summary. 
  #' wrapper for the base::summary function on model objects
  #' Can also use as head for better display
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param modelsmmrytable This can be a generic table, a model object such as lm(), or the summary of a model object summary(lm()) 
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return HTML table for display
  #' @examples
  #' library("xtable")
  #' library("kableExtra")
  #' xkabledply( df, title="Table testing", pos="left", bso="hover" )
  #' xkabledply( ISLR::Hitters[1:5,] )
  if (wide) { modelsmmrytable <- t(modelsmmrytable) }
  modelsmmrytable %>%
    xtable() %>% 
    kable(caption = title, digits = digits) %>%
    kable_styling(bootstrap_options = bso, full_width = FALSE, position = pos)
}

xkabledplyhead = function(df, rows=5, title="Head", digits = 4, pos="left", bso="striped") { 
  xkabledply(df[1:rows, ], title, digits, pos, bso, wide=FALSE)
}

xkabledplytail = function(df, rows=5, title="Tail", digits = 4, pos="left", bso="striped") { 
  trows = nrow(df)
  xkabledply(df[ (trows-rows+1) : trows, ], title, digits, pos, bso, wide=FALSE)
}

xkablesummary = function(df, title="Table: Statistics summary.", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display numeric variable summary of dataframes. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param df The dataframe.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return The HTML summary table for display, or for knitr to process into other formats 
  #' @examples
  #' xkablesummary( faraway::ozone )
  #' xkablesummary( ISLR::Hitters, title="Five number summary", pos="left", bso="hover"  )
  
  s = summary(df) %>%
    apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% # replace all leading words
    apply( 2, function(x) stringr::str_trim(x, "right")) # trim trailing spaces left
  
  colnames(s) <- stringr::str_trim(colnames(s))
  
  if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max') 
  } else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
  
  xkabledply(s, title=title, digits = digits, pos=pos, bso=bso )
}


xkablevif = function(model, title="VIFs of the model", digits = 3, pos="left", bso="striped", wide=TRUE) { 
  #' Combining faraway::vif, xtable, and kableExtra, to easily display numeric summary of VIFs for a model. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param model The lm or compatible model object.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return The HTML summary table of the VIFs for a model for display, or for knitr to process into other formats 
  #' @examples
  #' xkablevif( lm(Salary~Hits+RBI, data=ISLR::Hitters, wide=T ) )

  df <- data.frame(vif(model))                            
  names(df) <- "VIF"
     
  if (wide) { df <- t(df) }
  xkabledply(df, title=title, digits = digits, pos=pos, bso=bso )    
}
```


```{r stats_libs}
loadPkg("leaps")
loadPkg("ggplot2")
loadPkg("gridExtra")
loadPkg("scales")
loadPkg("heatmaply")
loadPkg("Hmisc")
loadPkg("faraway")
loadPkg("jtools")
```


```{r import_data}

data.full <- read.csv("melb_data.csv")

# create train and test data
samps <- read.csv("sample.txt", sep=" ")

data.train <- data.full[(samps$x),]
data.test <- data.full[(-samps$x),]

```


```{r create_ggplot_labels}
# factor level counts for legend
counts <- function(variable){                      
                                  
  table.temp <- table(variable)
  regions <- c(rep(0, length(table.temp)))                                            
  
  for (i in 1:length(table.temp)){
    name.new <- paste(names(table.temp[i]), table.temp[[i]], sep=" - ")
    regions[i] <- name.new
}
                                     
  return(regions)
}

```


# Introduction
Housing prices are an important indicator of the strength of the economy. House price prediction can help real estate developers determine the selling price of a house, allow buyers make informed choices about potential purchases, and be beneficial for property investors in determining price trends across different locations. 
Hence having a simple predictive and inferential method to model housing prices can be of great significance to the financial market; however, predicting long-term housing prices has become a complex and challenging task. 
This paper discusses our project on determining how different factors may affect home sales price by building linear models. 
The data used in this project was collected in Melbourne, Australia in 2017. 
Melbourne is a large metropolitan city with a strong real estate market in a region of Australia that experienced a 4.2 percent growth rate in property sales 2017. 
We believe the factors that determine housing pricing in our model could have broad applications to other locations and countries.
 
Our project sought to answer the following questions:

- Understand if housing prices in Melbourne, Australia can be predicted using this dataset.
- Determine what variables have the greatest impact on housing price.
- Analyze the impacts of location, seller, and construction attributes of homes on the housing market in Melbourne, Australia.
 

## The Melbourne Housing Snapshot Dataset
The independent variables mainly reflect the situation of the house from three dimensions: a. what type; b. quality, grade; c. quantity, area. Before exploratory data analysis, the details and introduction of the existing Melbourne house data variables are as follows:

```{r data_feats}                                                                                                                                                                                 

nvar <- ncol(data.full)
nObs <- nrow(data.full)

numerics <- c("Rooms", "Price", "Distance", "Bedroom2", "Bathroom", 
              "Car", "Landsize", "BuildingArea", "YearBuilt", 
              "Lattitude", "Longtitude",
              "Propertycount")                             

numerics.fixed <- c("Rooms", "Price", "Distance", "Bedrooms", "Bathrooms", 
              "Car Parks", "Land size", "Building Area", "Year Built", 
              "Lattitude", "Longtitude",
              "Property Count")
                       
data.fixedname <- data.full                                     
colnames(data.fixedname)[colnames(data.fixedname) %in% numerics] <- numerics.fixed

```

- Home Sales in 2017
  - Location                              
  - Construction
  - Sale
- Variables: `r nvar`
  - Numeric: `r length(numerics)`
  - Categorical: `r nvar - length(numerics)`
                                                           
```{r vars, results="markdown"}
var.names <- c(names(data.full))

```

Rooms: Number of rooms
                                                                                          
Price: Price (AUS$)

Method: Method of sale - 5 categories      

Type:  House, Unit, Townhouse - 3 categories   

SellerG: Real Estate Agent - 268 categories             

Date: Date sold                                                                                 

Distance: Distance from Central Business District

Regionname: Region name - 8 categories

Propertycount: Number of properties that exist in the suburb
                                                                    
Bedroom2 : Number of Bedrooms                              
                                                        
Bathroom: Number of Bathrooms                                                                                                    

Car: Number of carspots                                                                                                   

Landsize: Land Size                             

BuildingArea: Building Size

YearBuilt: Year home built

CouncilArea: Governing council for the area - 34 categories

Lattitude, Longtitude: GPS location

Suburb: Suburb name - 314 categories                                            

# Exploratory Data Analysis (EDA)
The following are excerpts and graphs from our exploratory data analysis. 
This part of the project familiarizes the reader with our dataset's attributes as well as lays the foundation for the variables we will include in our linear model. 
The results of our EDA will also inform the future direction of the project. 


## Summary of Price Statistics

SD: `r sqrt(var(data.full$Price))`

`r xkablesummary( as.data.frame(data.full$Price), title=NULL)`

## Select Data Pairs

By these scatter plots of selected numerical variables, we can see there are couple of potential outliers when it comes to home size, land size, and selling price. This does make it a bit difficult to discern patterns in some of the pairings. Nonetheless, there does appear to be an inkling of linear correlation which we will explore further. 

```{r pairs}
pairs(data.full[numerics[c(2, 3, 7, 8, 12)]])
```

## Corrleations
Due to the large number of feature columns in the dataset, it is difficult to grasp all the pairings in linear correlations. Therefore, before further feature mining, take a look at which variables are highly correlated with House Price. The below heatmap is interactive if viewing in HTML. Hover over the specific coordinate to view the correlation coefficient and the p-value for that variable pair. 

```{r heatmap}

data.corr <- rcorr(as.matrix(data.fixedname[numerics.fixed]))

hm <- heatmaply_cor(
  data.corr$r,
  main="Correlation Heatmap for Numeric Variables",
  custom_hovertext=matrix(paste("p-value:", round(data.corr$P, digits=4)), nrow=length(numerics.fixed)),
  dendrogram="none",
  grid_color="black",
  label_names = c("x", "y", "Correlation")
)

```
`r hm`

The general trajectory of most of these correlations should be unsurprising. One would expect a home with more rooms to be positively correlated with both number of bedrooms and price. Alternatively, distance from the central business district is slightly negatively correlated with price. And there are also strong correlations between some variables, such as Rooms and Bedrooms, Rooms and Bathrooms, Bedrooms and Bathrooms, which may be a concern for multicollinearity. This will guide our feature selection for linear regression and bear out in the significance and variance inflation within the attempted models. 

## Map of Melbourne Sales
A visual analysis of the current housing sales distribution in Melbourne was carried out. The result is shown in the figure below.

```{r price_region_qq}


ggplot(data.full) + 
  aes(x=Lattitude, y=Longtitude, color=Regionname) + 
  geom_point()

```

It can be seen from the figure that the sales areas are mainly concentrated in Eastern metropolitan, Southern metropolitan, Northern Metropolitan, Western Metropolitan and South-Eastern metropolitan. Therefore, the fluctuation of housing prices will greatly affect these areas, and these areas account for about 5/6 of Melbourne.


## Selling Price
Selling price is the variable that we would like to predict and infer upon by linear modeling. So, let us further explore its distribution. 

```{r price_data_plots}


layout(matrix(c(1,2,3), 1,3))
hist(data.full$Price, col = "blue", xlab="Price (AUS$)", main=NULL)
boxplot(data.full$Price, col = "red", ylab="Price (AUS$)")
qqnorm(data.full$Price, col = "dark green", ylab="Price (AUS$)")
qqline(data.full$Price)

```

By histogram, box-plot, and qq-plot, selling price appears skewed from normal. This makes sense as no sales were less than \$85,000 and there is a theoretical hard stop on the left at \$0. Similarly, housing prices can be quite high without theoretical limit. That pattern is clearly displayed here. 

The log transformed price is normally distributed; this can be seen in the histogram, boxplot, and qq-plot; all showing only very slight deviation from normal. Hence, selling price is log-normal. 


```{r log_price_plots}
layout(matrix(c(1,2,3), 1,3))
hist(log(data.full$Price), col = "blue", xlab="log(Price)", main=NULL)
boxplot(log(data.full$Price), col = "red", ylab="log(Price)")
qqnorm(log(data.full$Price), col = "dark green", ylab="log(Price)")
qqline(log(data.full$Price))

logPrice.ttest <- t.test(log(data.full$Price), conf.level=0.95)

Price.lower <- exp(logPrice.ttest$conf.int[1])
Price.upper <- exp(logPrice.ttest$conf.int[2])

```



### Selling Price's Relationship to Select Categorical Variables

We suspect location, seller, and type of home interacts with sale price. Furthermore, by the above heat-map, the price is correlated to number of rooms which can be treated as categorical; the more rooms there are, the higher the price.

### Price by Region

By box-plot, there does appear to be an dependence between region and price. Note that the data still appear non-normal and suspiciously log-normal. Although the number of observations in each level of region are sufficiently large, there is great variability in the size of each level. Western Victoria has only 32 sales but Southern Metropolitan had 4695. Since normality is not satisfied and with uneven sized levels, we cannot rely on the robustness of ANOVA to test for independence.  

```{r price_region}

region.count <- counts(data.full$Regionname)

ggplot(data.full, 
       aes(x=Regionname, y=Price, fill=Regionname)) + 
  geom_boxplot() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + 
  labs(y="Price (AUS$)") + 
  scale_fill_discrete(name="Region Name", labels=region.count) +
  scale_y_continuous(labels = scales::comma)

```


### Price by Number of Rooms (<10 Rooms)

Due to the small size of levels of rooms greater than 9, they are omitted. Again, we can see data that one can suspect is log-normal. There appears to be in dependence based on the box-plot especially for homes with less than six rooms; this is less pronounced as number of rooms increases. Again, the size of the levels is highly uneven; hence, we cannot apply ANOVA testing for dependence in this case either.  

```{r price_rooms}

rooms.count <- counts(data.full$Rooms)

ggplot(subset(data.full, Rooms<10), 
       aes(x=factor(Rooms), y=Price, fill=factor(Rooms))) + 
  geom_boxplot() + 
  labs(x="Number of Rooms", y="Price (AUS$)") + 
  scale_y_continuous(labels = scales::comma) +
  scale_fill_discrete(name="Number of Rooms - Obs", labels=rooms.count) 

```


### Price by Type of Home

The pattern of non-normal data and uneven level sizes repeats when price is conditioned on type of home (house, apartment, townhouse). So, again, ANOVA is not appropriate. 

```{r price_type}

type.count <- counts(data.full$Type)

ggplot(data.full, 
       aes(x=factor(Type), y=Price, fill=Type)) + 
  geom_boxplot() + 
  labs(y="Price (AUS$)") + 
  scale_y_continuous(labels = scales::comma) +
  guides(fill=FALSE) + 
  scale_x_discrete(name="Type of Home", labels=c("House", "Townhouse", "Apartment"))

```


### Test of Independence by Group (Pearson $\chi^2$)

Since ANOVA testing for independence is inadvisable given the data distribution of selling price, Pearson $\chi^2$ testing is implemented. For this, price must be categorical; so, it is split into five uneven groups. This is done so that the highest price level have enough observations for the Pearson assumptions but does not include a large number of average priced homes. The variables being tested are seller (SellerG), number of rooms (Rooms), region (Regionname), and type of home (Type). 

```{r type_person}

data.full$Price.cuts <- as.character(cut(data.full$Price, 10))
data.full$Price.cuts[data.full$Price > 3650000] <- "(3.65e+06,9.01e+06]"

data.full$Price.cuts <- as.factor(data.full$Price.cuts)


type.chi <- chisq.test(data.full$Type, data.full$Price.cut)
rooms.chi <- chisq.test(subset(data.full, data.full$Rooms<10)$Rooms, subset(data.full, data.full$Rooms<10)$Price.cut)
region.chi <- chisq.test(data.full$Regionname, data.full$Price.cut)

seller.chi <- chisq.test(data.full$SellerG, data.full$Price.cut)
```

$H_0$: All means equal by group 

All reject $H_0$ with p-value$<2\times 10^{-16}$

The null hypothesis is that there is no difference in means among each level of the four categorical variables that we test. As expected, we reject the null at $\alpha=0.05$. We conclude that in each categorical variable, some level has at least one mean that is not equal to the the other levels. This is expected from both basic knowledge of housing markets and the box-plots above. 


## Price by Region and Type
The picture below shows the price by region and type of home. Many of the same patterns hold. 

```{r price_type_region}

p1 <- ggplot(subset(data.full, Type=="h"), 
       aes(x=Regionname, y=Price, fill=Regionname)) + 
  geom_boxplot() + 
  labs(title="Houses", y="Price (AUS$)") + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  scale_y_continuous(labels = scales::comma) +
  guides(fill=FALSE)




p2<- ggplot(subset(data.full, Type=="t"), 
       aes(x=Regionname, y=Price, fill=Regionname)) + 
  geom_boxplot() + 
  labs(title="Townhouses", y=NULL) + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  scale_y_continuous(labels = scales::comma) +
  guides(fill=FALSE)



p3 <- ggplot(subset(data.full, Type=="u"), 
       aes(x=Regionname, y=Price, fill=Regionname)) + 
  geom_boxplot() + 
  labs(title="Apartments", y=NULL) + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_y_continuous(labels = scales::comma) +
  guides(fill=FALSE)

grid.arrange(p1,p2,p3, nrow=1)

```


# Linear Modelling

For building a ordinary least squares (OLS), 70% of the dataset is randomly selected as training and 30% is used for final testing. Upon testing on the validation set, the model is no longer altered. 


## First Attempt at Linear Model
In this first model, we set the regression model as: 
Price ~ Rooms + Landsize + Distance + Bedroom2 + Bathroom + Car + BuildingArea + Lattitude + Longtitude + Propertycount + factor (Regionname).
In order to determine whether the first model meets the requirements, the necessary variance inflation factor (VIF) checks are useful; we follow the convention to consider removal of variables with  VIF$>5$. From the plot below, we see that the bedroom2 is not suitable for house price regression analysis due to its high VIF. Thus, the second OLS regression model is modified with no bedroom2 variable.

```{r lm1, results='markdown'}

mod <- lm(Price ~ Rooms + Landsize
          + Distance + Bedroom2 + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train)

mod.vifs <- vif(mod)

plot(mod.vifs,
     xlab="", ylab="VIF",
     xaxt='n',
     main="VIF Values for First Model")
abline(h=5, col="blue")

points(which.max(mod.vifs), max(mod.vifs), pch=23, col="red", bg="red")
text(which.max(mod.vifs), max(mod.vifs)-1, label=names(mod$coefficients[which.max(mod.vifs)+1]))

```


## Linear Model 2: Removed the Variable with Highest VIF
In this model, we set the regression model as: 
Price ~ Rooms + Landsize + Distance + Bathroom + Car + BuildingArea + Lattitude + Longtitude + Propertycount + factor (Regionname). 
Similarly with Model 1, the VIF’s results shown below. As expected, we get essentially the same plot as before but note that a single level of region (Regionname) exceeds the VIF cutoff. We choose to keep it as all other levels are below the cutoff; furthermore, the VIF does not exceed 5 by some great amount. Hence Regionname is kept. 

```{r lm2_vif, results='markdown'}
# drop Bedroom2 - keep
mod2 <-  lm(Price ~ Rooms + Landsize
          + Distance  + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train)


mod2.vifs <- vif(mod2)

plot(mod2.vifs,
     xlab="", ylab="VIF",
     xaxt='n',
     main="VIF Values for Second Model")
abline(h=5, col="blue")

points(which.max(mod2.vifs), max(mod2.vifs), pch=23, col="red", bg="red")
text(which.max(mod2.vifs)-3, max(mod2.vifs)-0.5, label=names(mod2$coefficients[which.max(mod2.vifs)+1]))

```

## Model Coefficients
In this model, the factor (Regionname) Western Metropolitan is the variable with highest VIF value. The p-values for the model coefficients are as shown below. Although, it may be a unusual to plot p-values for coefficients, this is done to quickly identify variables that may require additional investigation. 


```{r lm2_mod, results='markdown'}

mod2.summ <-summary(mod2)
plot(mod2.summ$coefficients[,"Pr(>|t|)"],
     xlab="", ylab="p-value",
     xaxt='n',                 
     main="p-values for Second Model")
abline(h=0.05, col="blue")

points(11:mod2$rank, mod2.summ$coefficients[,"Pr(>|t|)"][11:mod2$rank], pch=21, col="blue", bg="blue")
points(3,mod2.summ$coefficients[,"Pr(>|t|)"][3], pch=23, col="red", bg="red")
text(3, mod2.summ$coefficients[,"Pr(>|t|)"][3]-0.01, label=names(mod2$coefficients[3]))

legend("topright",
       pch=21,
       pt.bg="blue",
       col="blue",
       legend="Region Factor Levels")
```

The p-value of Landsize is the highest, which is larger than our assigned $\alpha=0.05$. Thus, this variable is dropped. Again, we choose to maintain Regionname as most levels are significant at this $\alpha$.


## Linear Model 3: Considered Interactions
In this model, we analyze the interaction of rooms and region. The result is shown as follows: 


```{r lm3, results='markdown'}
#  mod 2 with rooms*region interaction -  failed model, drop interaction
mod3 <-  lm(Price ~ Rooms + Landsize
          + Distance  + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname)*Rooms , data=data.train)


mod3.summ <-summary(mod3)                                                                                          
plot(mod3.summ$coefficients[,"Pr(>|t|)"],
     xlab="", ylab="p-value",                                                                                              
     xaxt='n',
     main="p-values for Model with Interaction")
abline(h=0.05, col="blue")

points(11:17, mod3.summ$coefficients[,"Pr(>|t|)"][11:17], pch=21, col="blue", bg="blue")
points(18:24, mod3.summ$coefficients[,"Pr(>|t|)"][18:24], pch=21, col="darkgreen", bg="darkgreen")

legend("topright",
       pch=21,
       pt.bg=c("blue", "darkgreen"),
       col=c("blue", "darkgreen"),
       legend=c("Region Factor Levels", "Interaction Rooms:Region"))



```

The p-value for the model suggests that the interaction is not significant and is discarded.  



## Linear Model 4: Removed Land Size and without Interaction
Consequently from the above model analysis, the variable of landsize should be dropped and the Rooms-Regionname variable. The model is set as:
Price ~ Rooms + Distance + Bathroom + Car + BuildingArea + Lattitude + Longtitude + Propertycount + factor (Regionname). 

```{r lm4_pvalues, results='markdown'}

mod4 <-  lm(Price ~ Rooms
          + Distance  + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname), data=data.train[!(rownames(data.train) %in% c("13246","2561", "1589")),])

mod4.summ <- summary(mod4)

plot(mod4.summ$coefficients[,"Pr(>|t|)"],
     xlab="", ylab="p-value",
     xaxt='n',
     main="p-values for Second Model")
abline(h=0.05, col="blue")                                                  

points(10:mod4$rank, mod4.summ$coefficients[,"Pr(>|t|)"][10:mod4$rank], pch=21, col="blue", bg="blue")


legend("topright",
       pch=21,
       pt.bg="blue",
       col="blue",
       legend="Region Factor Levels")


```

Again, we see a few p-values that exceed our $\alpha$ but these are just a few levels of Regionname which we have decided to keep. It is also important to note that $R^2_{adj}$ was generally stable around 0.55 with all of the above models. 


# Residual Analysis

## Homogeneity? No     

There does appear to be a slight yet notable cone shape of the residual verus fitted values, suggesting some heterogeneity of variance. A transform may be considered. 

```{r lm4_redids}

plot(mod4, which=1, sub.caption="Model 4")
```

## Normal? Not Quite

By qq-plot, there does appear some deviation from normality of the standardized residuals. Again, transformation or alternative modeling may be considered. 

```{r lm4_qq}

plot(mod4, which=2, sub.caption="Model 4")                                                                                                                                                      
```

## Influence? Yes

There does appear to be one high influence point with leverage nearing 1 and an outlier of more than 10 standard deviations. 

```{r lm4_lev}

plot(mod4, which=5, sub.caption="Model 4")
```



## Remove Influence Points

Removal of the influence point is considered but does not greatly alter coefficients, their significance, or $R^2_{adj}$. Since the model is not so different, and the reason for high influence not sufficiently understood, the observations are not removed from the final model. 

```{r lm5}
                               
# drop drop influence points
mod5 <-  lm(Price ~ Rooms
          + Distance  + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname), data=data.train[!(rownames(data.train) %in% c("13246","2561", "1589")),])

plot(mod4, which=5, sub.caption="Model 4 without Outliers")
```




## Transform Data - Homogeneity                                                    
This is a residual analysis on the transformed log-Price linear model: log(Price) ~ Rooms + Distance + Bathroom + Car + BuildingArea + Lattitude + Longtitude + Propertycount + factor (Regionname). 

```{r lm5_trans}
# log transform
# drop drop influence points
mod6 <- lm(log(Price) ~ Rooms + Car
          + Distance   + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train[!(rownames(data.train) %in% c("13246","2561", "1589")),])

plot(mod6, which=1, sub.caption="Model 4 with Log(Price) Transformation")


y.trans.pred<-predict(mod6)

```

We do see improvement of the cone shape and now no discernable pattern appears. 

## Transform Data - Normality    


```{r lm5_trans_qq}
# log transform
# drop drop influence points
mod6 <- lm(log(Price) ~ Rooms + Car
          + Distance   + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train[!(rownames(data.train) %in% c("13246","2561", "1589")),])

plot(mod6, which=2, sub.caption="Model 4 with Log(Price) Transformation")
```

The qq-plot shows some mild improvement using the transformed data. With the robustness of OLD regression, we will not include the log-transform model knowing that it will also lose interpretibilty for inferential purposes. 

                                                                                                       
# Proposed Model 

This is the final proposed model and details of related coefficients are shown as follows:

``` {r prop_mod, results="markdown"}
summ(mod4)
```

Although the residual analysis is somewhat concerning, we rely on the robustness of OLS regression to recommend this model. No influence points are omitted. There are, however, a large number of missing values for many variables.   


## Testing $R^2$

```{r lm4_pred}
y.pred <- predict(mod4, data.test)                                        
y.test <- data.test$Price                          

diff.sq = (y.pred - y.test)^2 

keeps <- which(!is.na(diff.sq))

r2 <- function(y.predict, y.actual=y.test){
  
  TSS <- sum((y.actual - mean(y.actual))^2)
  RSS <- sum((y.predict - y.actual)^2)
  
  rSq <- 1 - RSS/TSS
  
  return(rSq)
}
    
r.2 <- r2(y.pred[keeps], y.test[keeps])                                           

```

An alternative $R^2$ is used to validate the model. This is done on the testing set:

$$ \begin{equation} R^2 = 1- \dfrac{RSS}{TSS} \end{equation}=0.441$$ 

This is roughly inline with what we would expect since the final model has $R^@_{adj}=0.56$. Note that the large number of missing data are simply omitted from this value. 




# Conclusion
The previous analysis did not deal with outliers, and the processing of outliers may also have a certain effect on result optimization. Through the analysis of this data set, the content of linear regression was practiced, and the final effect was interpretable even if it does not excel at prediction. For this, alternative modeling methods must be considered. A more granular analysis of price and dependency would also be informative. Furthermore, a large number of missing values hinders model building. 

## Future Work
- Further explore log transformation                 
- Consider GLM with log link
- Categorical variables with many levels
- Missing data
- Improve Prediction 

# 8 Bibliography 
Dataset available: https://www.kaggle.com/dansbecker/melbourne-housing-snapshot

Thorne,S. (2019, November 3) How the Australian Property Market Performed in 2017. Retrieved from www.openagent.com.au/blog/how-the-australian-property-market-performed-in-2017#. 

Mansfield, E. R., & Helms, B. P. (1982). Detecting multicollinearity. The American Statistician, 36(3a), 158-160.

Daoud, J. I. (2017, December). Multicollinearity and regression analysis. In Journal of Physics: Conference Series (Vol. 949, No. 1, p. 012009). IOP Publishing.

Brownie, Cavell, and Dennis D. Boos. (1994). Type I Error Robustness of ANOVA and ANOVA on Ranks When the Number of Treatments Is Large. Biometrics, vol. 50, no. 2, 1994, p. 542.

Lix, Lisa M., et al. (1996). Consequences of Assumption Violations Revisited: A Quantitative Review of Alternatives to the One-Way Analysis of Variance ‘F’ Test. Review of Educational Research, vol. 66, no. 4, 1996, p. 579.

