---
title: "Melbourne Housing Market-Paper1"
subtitle: "Intro Data Science"
author: "Team 2"
date: "`r Sys.Date()`"
output:
   html_document:
     toc: true
     toc_float: true
     number_section: true
---
# Introduction

The buying and selling of houses has always been the most concerned issue of the people, and it is a major issue related to people's livelihood. Housing prices are an important indicator that reflects the performance of the economy. 
Real estate developers and home buyers pay close attention to housing price fluctuations. 
In a complete market environment, housing prices are jointly determined by demand and supply. 
However, in the current society, housing has both residential and investment functions, making the research on housing prices very complicated. 
To predict long-term housing prices has become an extremely complex and challenging task. The establishment of a real estate price prediction system is a key task for the healthy development of the current real estate industry. 
However, the rapid rise in housing prices has always plagued most of the middle and low-income groups, and this has developed into a sensitive issue in people's daily life. 
Therefore, studying the influencing factors of real estate prices and predicting them will help the people of the middle and low-income groups to choose the right time to buy a house, and it will also help the government officials understand the trend of real estate prices and regulate them. 
Having a simple predictive and inferential method to model housing prices helps commerce determine fair prices and allows governments to determine property taxes. This project aims to learn how different factors may affect home sales price by building linear models. Although data that will be utilized was collected in Melbourne, Australia in 2017, the concept that location and home attributes correlate with housing prices could reasonably apply broadly and internationally. 

# Introduction Re-Write:
Housing prices are an important indicator of the strength of the economy. House price prediction can help real estate developers determine the selling price of a house, allow buyers make informed choices about potential purchases, and be beneficial for property investors in determining price trends across different locations. 
Hence having a simple predictive and inferential method to model housing prices can be of great significance to the financial market; however, predicting long-term housing prices has become a complex and challenging task. 
This paper discusses our project on determining how different factors may affect home sales price by building linear models. 
The data used in this project was collected in Melbourne, Australia in 2017. 
Melbourne is a large metropolitan city with a strong real estate market in a region of Australia that experienced a 4.2 percent growth rate in property sales 2017. 
We believe the factors that determine housing pricing in our model could have broad applications to other locations and countries.
 
Our project sought to answer the following questions:
1. Understand if housing prices in Melbourne, Australia can be predicted using this dataset.
2. Determine what variables have the greatest impact on housing price.
3. Analyze the impacts of location, seller, and construction attributes of homes on the housing market in Melbourne, Australia.
 



```{r basic, include=F}
# use this function to conveniently load libraries and work smoothly with knitting
# can add quietly=T option to the require() function
# the loadPkg function essentially replaced/substituted two functions install.packages() and library() in one step.
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }

# unload/detact package when done using it
unloadPkg = function(pkg, character.only = FALSE) { 
  if(!character.only) { pkg <- as.character(substitute(pkg)) } 
  search_item <- paste("package", pkg,sep = ":") 
  while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) } 
}
```

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
knitr::opts_chunk$set(echo = F)
options(scientific=T, digits = 5) 
options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times

loadPkg("leaps")
loadPkg("ggplot2")
loadPkg("gridExtra")
loadPkg("scales")
loadPkg("heatmaply")
loadPkg("Hmisc")
loadPkg("faraway")
loadPkg("jtools")

set.seed(17)
data.full <- read.csv("melb_data.csv")

# create train and test data
samps <- read.csv("sample.txt", sep=" ")

data.train <- data.full[(samps$x),]
data.test <- data.full[(-samps$x),]

```

```{r xkablesummary}
loadPkg("xtable")
loadPkg("kableExtra")
loadPkg("stringi")

xkabledply = function(modelsmmrytable, title="Table", digits = 4, pos="left", bso="striped", wide=FALSE) { 
  #' Combining base::summary, xtable, and kableExtra, to easily display model summary. 
  #' wrapper for the base::summary function on model objects
  #' Can also use as head for better display
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param modelsmmrytable This can be a generic table, a model object such as lm(), or the summary of a model object summary(lm()) 
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return HTML table for display
  #' @examples
  #' library("xtable")
  #' library("kableExtra")
  #' xkabledply( df, title="Table testing", pos="left", bso="hover" )
  #' xkabledply( ISLR::Hitters[1:5,] )
  if (wide) { modelsmmrytable <- t(modelsmmrytable) }
  modelsmmrytable %>%
    xtable() %>% 
    kable(caption = title, digits = digits) %>%
    kable_styling(bootstrap_options = bso, full_width = FALSE, position = pos)
}

xkabledplyhead = function(df, rows=5, title="Head", digits = 4, pos="left", bso="striped") { 
  xkabledply(df[1:rows, ], title, digits, pos, bso, wide=FALSE)
}

xkabledplytail = function(df, rows=5, title="Tail", digits = 4, pos="left", bso="striped") { 
  trows = nrow(df)
  xkabledply(df[ (trows-rows+1) : trows, ], title, digits, pos, bso, wide=FALSE)
}

xkablesummary = function(df, title="Table: Statistics summary.", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display numeric variable summary of dataframes. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param df The dataframe.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return The HTML summary table for display, or for knitr to process into other formats 
  #' @examples
  #' xkablesummary( faraway::ozone )
  #' xkablesummary( ISLR::Hitters, title="Five number summary", pos="left", bso="hover"  )
  
  s = summary(df) %>%
    apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% # replace all leading words
    apply( 2, function(x) stringr::str_trim(x, "right")) # trim trailing spaces left
  
  colnames(s) <- stringr::str_trim(colnames(s))
  
  if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max') 
  } else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
  
  xkabledply(s, title=title, digits = digits, pos=pos, bso=bso )
}


xkablevif = function(model, title="VIFs of the model", digits = 3, pos="left", bso="striped", wide=TRUE) { 
  #' Combining faraway::vif, xtable, and kableExtra, to easily display numeric summary of VIFs for a model. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param model The lm or compatible model object.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return The HTML summary table of the VIFs for a model for display, or for knitr to process into other formats 
  #' @examples
  #' xkablevif( lm(Salary~Hits+RBI, data=ISLR::Hitters, wide=T ) )

  df <- data.frame(vif(model))                            
  names(df) <- "VIF"
     
  if (wide) { df <- t(df) }
  xkabledply(df, title=title, digits = digits, pos=pos, bso=bso )    
}
```

```{r outlierKD2}       
# Fix outliers
outlierKD2 <- function(df, var, rm=FALSE, boxplt=FALSE, histogram=TRUE, qqplt=FALSE, variable_name=deparse(substitute(var))) { 
    #' Original outlierKD functino by By Klodian Dhana,
    #' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
    #' Modified to have third argument for removing outliers instead of interactive prompt, 
    #' and after removing outlier, original df will not be changed. The function returns the a df, 
    #' which can be saved as original df name if desired.
    #' Also added QQ-plot in the output, with options to show/hide boxplot, histogram, qqplot.                                 
    #' Check outliers, and option to remove them, save as a new dataframe. 
    #' @param df The dataframe.                                          
    #' @param var The variable in the dataframe to be checked for outliers
    #' @param rm Boolean. Whether to remove outliers or not.          
    #' @param boxplt Boolean. Whether to show the boxplot, before and after outliers removed.
    #' @param histogram Boolean. Whether to show the histogram, before and after outliers removed.
    #' @param qqplt Boolean. Whether to show the qqplot, before and after outliers removed.                                   
    #' @param variable_name String. Variable name formatted for title.
    #' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed               
    #' @examples    
    #' outlierKD2(mydf, height, FALSE, TRUE, TRUE, TRUE)
    #' mydf = outlierKD2(mydf, height, TRUE, TRUE, TRUE, TRUE)
    #' mydfnew = outlierKD2(mydf, height, TRUE)
    dt = df # duplicate the dataframe for potential alteration
    var_name <- eval(substitute(var),eval(dt))     
    na1 <- sum(is.na(var_name))
    m1 <- mean(var_name, na.rm = T)
    par(mfrow=c(2, boxplt+histogram+qqplt), oma=c(0,0,3,0))
    if (qqplt) { 
      qqnorm(var_name, main = "With outliers")
      qqline(var_name)
    }
    if (histogram) { hist(var_name, main="With outliers", xlab=NA, ylab=NA) }
    if (boxplt) { boxplot(var_name, main="With outliers") }

    outlier <- boxplot.stats(var_name)$out
    mo <- mean(outlier)
    var_name <- ifelse(var_name %in% outlier, NA, var_name)
    if (qqplt) { 
      qqnorm(var_name, main = "Without outliers")
      qqline(var_name)
    }
    if (histogram) { hist(var_name, main="Without outliers", xlab=NA, ylab=NA) }
    if (boxplt) { boxplot(var_name, main="Without outliers") }

    title(paste("Outlier Check:", variable_name), outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
    
    # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
    # if(response == "y" | response == "yes"){
    if(rm){
        dt[as.character(substitute(var))] <- invisible(var_name)
        #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
        cat("Outliers successfully removed", "\n")
        return(invisible(dt))
    } else {
        cat("Nothing changed", "\n")
        return(invisible(df))
    }
}
```

```{r create_ggplot_labes}
# factor level counts for legend
counts <- function(variable){                      
                                  
  table.temp <- table(variable)
  regions <- c(rep(0, length(table.temp)))                                            
  
  for (i in 1:length(table.temp)){
    name.new <- paste(names(table.temp[i]), table.temp[[i]], sep=" - ")
    regions[i] <- name.new
}
                                     
  return(regions)
}

```
# Exploratory Data Analysis (EDA)
The following are excerpts and graphs from our exploratory data analysis. 
This part of the project familiarizes the reader with our dataset's attributes as well as lays the foundation for the variables we will include in our linear model. 
The results of our EDA will also inform the future direction of the project. We completed our EDA with the folowing goals in mind:

## Goals
1.	Understand which attributes of a home and its sale determine final sale price
2.	Attempt to build a reasonable model for inference and/or prediction for final sale price



## The Melbourne Housing Snapshot Dataset
The independent variables mainly reflect the situation of the house from three dimensions: a. what type; b. quality, grade; c. quantity, area.Before EDA, the details and introduction of the existing Melbourne house data variables are as follows:

```{r data_feats}                                                                                                                                                                                 

nvar <- ncol(data.full)
nObs <- nrow(data.full)

numerics <- c("Rooms", "Price", "Distance", "Bedroom2", "Bathroom", 
              "Car", "Landsize", "BuildingArea", "YearBuilt", 
              "Lattitude", "Longtitude",
              "Propertycount")                             

numerics.fixed <- c("Rooms", "Price", "Distance", "Bedrooms", "Bathrooms", 
              "Car Parks", "Land size", "Building Area", "Year Built", 
              "Lattitude", "Longtitude",
              "Property Count")
                       
data.fixedname <- data.full                                     
colnames(data.fixedname)[colnames(data.fixedname) %in% numerics] <- numerics.fixed

```

- Home Sales in 2017
  - Location                              
  - Construction
  - Sale
- Variables: `r nvar`
  - Numeric: `r length(numerics)`
  - Categorical: `r nvar - length(numerics)`
                                                           
## The Variables
```{r vars, results="markdown"}
var.names <- c(names(data.full))

```

Rooms: Number of rooms
                                                                                          
Price: Price (AUS$)

Method: Method of sale - 5 categories      

Type:  House, Unit, Townhouse - 3 categories   

SellerG: Real Estate Agent - 268 categories             

Date: Date sold                                                                                 

Distance: Distance from Central Business District

Regionname: Region name - 8 categories

Propertycount: Number of properties that exist in the suburb
                                                                    
Bedroom2 : Number of Bedrooms                              
                                                        
Bathroom: Number of Bathrooms                                                                                                    

Car: Number of carspots                                                                                                   

Landsize: Land Size                             

BuildingArea: Building Size

YearBuilt: Year home built

CouncilArea: Governing council for the area - 34 categories

Lattitude, Longtitude: GPS location

Suburb: Suburb name - 314 categories                                            

## Summary of Price Statistics

Mean: `r dollar(mean(data.full$Price))`

SD: `r sqrt(var(data.full$Price))`

`r xkablesummary( as.data.frame(data.full$Price), title=NULL)`

## Select Data Pairs
```{r pairs}
pairs(data.full[numerics[c(2, 3, 7, 8, 12)]])
```

## Corrleations
Due to the large number of feature columns in the dataset, it is difficult to grasp. Therefore, before further feature mining, take a look at which variables are highly correlated with House Price (mainly focusing on numeric variables). The result is shown as follows:
```{r heatmap}

data.corr <- rcorr(as.matrix(data.fixedname[numerics.fixed]))

hm <- heatmaply_cor(
  data.corr$r,
  main="Correlation Heatmap for Numeric Variables",
  custom_hovertext=matrix(paste("p-value:", round(data.corr$P, digits=4)), nrow=length(numerics.fixed)),
  dendrogram="none",
  grid_color="black",
  label_names = c("x", "y", "Correlation")
)

```
`r hm`

We can draw conclusion from the above picture that the price has a positive relationship with the type of room, but has a negative relationship with distance. And there are also strong correlations between some variables, such as Rooms and Bedrooms, Rooms and Bathrooms, Bedrooms and Bathrooms, which may cause multicollinearity. We need to pay attention to feature selection.

## Selling Price
In order to forecast the sales price more reasonably, before making the forecast, there is a comparative analysis of its distribution. SalePrice is a predictor variable. Take a graph to see its distribution:

```{r price_data_plots}


layout(matrix(c(1,2,3), 1,3))
hist(data.full$Price, col = "blue", xlab="Price (AUS$)", main=NULL)
boxplot(data.full$Price, col = "red", ylab="Price (AUS$)")
qqnorm(data.full$Price, col = "dark green", ylab="Price (AUS$)")
qqline(data.full$Price)

```

Each point of a string of numbers in the data is a certain quantile of the data. Pair these points (called sample quantile points) with the corresponding theoretical quantiles to make a scatter plot. The data obeys the normal distribution, then the graph should look like a straight line, otherwise it does not obey the normal distribution. It can be seen from the distribution diagram of selling price that selling price does not follow a normal distribution. In the case of not obeying the normal distribution, a new method should be selected to change the data to obey the normal distribution. Thus, the log operation is implemented in selling price.

## Log Selling Price

```{r log_price_plots}
layout(matrix(c(1,2,3), 1,3))
hist(log(data.full$Price), col = "blue", xlab="log(Price)", main=NULL)
boxplot(log(data.full$Price), col = "red", ylab="log(Price)")
qqnorm(log(data.full$Price), col = "dark green", ylab="log(Price)")
qqline(log(data.full$Price))

logPrice.ttest <- t.test(log(data.full$Price), conf.level=0.95)

Price.lower <- exp(logPrice.ttest$conf.int[1])
Price.upper <- exp(logPrice.ttest$conf.int[2])

```

From the Q-Q plot of log selling price, we could draw that the graph looks like a straight line. Thus, log selling price obeys normal distribution.

## Map of Melbourne Sales

```{r price_region_qq}


ggplot(data.full) + 
  aes(x=Lattitude, y=Longtitude, color=Regionname) + 
  geom_point()

```

It can be seen from the figure that the sales areas are mainly concentrated in Eastern metropolitan, Southern metropolitan, Northern Metropolitan, Western Metropolitan and South-Eastern metropolitan. Therefore, the fluctuation of housing prices will greatly affect these areas, and these areas account for about 5/6 of Melbourne.

In the conclusion of the correlation analysis, the price is related to the region. In other words, the sale price of the house is also different in different regions. In addition, the price is also related to the number of rooms; the more rooms there are, the higher the price will be. Finally, housing prices are also related to the type of house. All the results are shown below:

## Price by Region

```{r price_region}

region.count <- counts(data.full$Regionname)

ggplot(data.full, 
       aes(x=Regionname, y=Price, fill=Regionname)) + 
  geom_boxplot() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + 
  labs(y="Price (AUS$)") + 
  scale_fill_discrete(name="Region Name", labels=region.count) +
  scale_y_continuous(labels = scales::comma)

```

## Price by Number of Rooms (<10 Rooms)

```{r price_rooms}

rooms.count <- counts(data.full$Rooms)

ggplot(subset(data.full, Rooms<10), 
       aes(x=factor(Rooms), y=Price, fill=factor(Rooms))) + 
  geom_boxplot() + 
  labs(x="Number of Rooms", y="Price (AUS$)") + 
  scale_y_continuous(labels = scales::comma) +
  scale_fill_discrete(name="Number of Rooms - Obs", labels=rooms.count) 

```


## Price by Type of Home

```{r price_type}

type.count <- counts(data.full$Type)

ggplot(data.full, 
       aes(x=factor(Type), y=Price, fill=Type)) + 
  geom_boxplot() + 
  labs(y="Price (AUS$)") + 
  scale_y_continuous(labels = scales::comma) +
  guides(fill=FALSE) + 
  scale_x_discrete(name="Type of Home", labels=c("House", "Townhouse", "Apartment"))

```


## Test of Independence by Group (Pearson $\chi^2$)
### Type, Rooms, Regionname, SellerG

```{r type_person}

data.full$Price.cuts <- as.character(cut(data.full$Price, 10))
data.full$Price.cuts[data.full$Price > 3650000] <- "(3.65e+06,9.01e+06]"

data.full$Price.cuts <- as.factor(data.full$Price.cuts)


type.chi <- chisq.test(data.full$Type, data.full$Price.cut)
rooms.chi <- chisq.test(subset(data.full, data.full$Rooms<10)$Rooms, subset(data.full, data.full$Rooms<10)$Price.cut)
region.chi <- chisq.test(data.full$Regionname, data.full$Price.cut)

seller.chi <- chisq.test(data.full$SellerG, data.full$Price.cut)
```

$H_0$: All means equal by group 


All reject $H_0$ with p-value$<2\times 10^{-16}$



## Price by Region and Type
The picture below shows the price with different region and type of house.

```{r price_type_region}

p1 <- ggplot(subset(data.full, Type=="h"), 
       aes(x=Regionname, y=Price, fill=Regionname)) + 
  geom_boxplot() + 
  labs(title="Houses", y="Price (AUS$)") + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  scale_y_continuous(labels = scales::comma) +
  guides(fill=FALSE)




p2<- ggplot(subset(data.full, Type=="t"), 
       aes(x=Regionname, y=Price, fill=Regionname)) + 
  geom_boxplot() + 
  labs(title="Townhouses", y=NULL) + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  scale_y_continuous(labels = scales::comma) +
  guides(fill=FALSE)



p3 <- ggplot(subset(data.full, Type=="u"), 
       aes(x=Regionname, y=Price, fill=Regionname)) + 
  geom_boxplot() + 
  labs(title="Apartments", y=NULL) + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_y_continuous(labels = scales::comma) +
  guides(fill=FALSE)

grid.arrange(p1,p2,p3, nrow=1)

```

## Transform Data - Homogeneity                                                    

```{r lm5_trans}
# log transform
# drop drop influence points
mod6 <- lm(log(Price) ~ Rooms + Car
          + Distance   + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train[!(rownames(data.train) %in% c("13246","2561", "1589")),])

plot(mod6, which=1, sub.caption="Model 4 with Log(Price) Transformation")


y.trans.pred<-predict(mod6)

```

## Transform Data - Normality                                                               
```{r lm5_trans_qq}
# log transform
# drop drop influence points
mod6 <- lm(log(Price) ~ Rooms + Car
          + Distance   + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train[!(rownames(data.train) %in% c("13246","2561", "1589")),])

plot(mod6, which=2, sub.caption="Model 4 with Log(Price) Transformation")
```


# Linear Modelling

## First Attempt at Linear Model
In this first model, we set the regression model as: 
Price ~ Rooms + Landsize + Distance + Bedroom2 + Bathroom + Car + BuildingArea + Lattitude + Longtitude + Propertycount + factor (Regionname).
In order to determine whether the first model meets the requirements, the necessary VIF checks are useful. From the picture, we can know that the bedroom2 is not suitable for house price regression analysis. Thus, the second model regression analysis is modified with no bedroom2 variable.

```{r lm1, results='asis'}

mod <- lm(Price ~ Rooms + Landsize
          + Distance + Bedroom2 + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train)

mod.vifs <- vif(mod)

plot(mod.vifs,
     xlab="", ylab="VIF",
     xaxt='n',
     main="VIF Values for First Model")
abline(h=5, col="blue")

points(which.max(mod.vifs), max(mod.vifs), pch=23, col="red", bg="red")
text(which.max(mod.vifs), max(mod.vifs)-1, label=names(mod$coefficients[which.max(mod.vifs)+1]))

```


## Linear Model 2: Removed the Variable with Highest VIF
In this model, we set the regression model as: 
Price ~ Rooms + Landsize + Distance + Bathroom + Car + BuildingArea + Lattitude + Longtitude + Propertycount + factor (Regionname). 
Similarly with Model 1, the VIF’s results shown as follows:

```{r lm2_vif, results='asis'}
# drop Bedroom2 - keep
mod2 <-  lm(Price ~ Rooms + Landsize
          + Distance  + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train)


mod2.vifs <- vif(mod2)

plot(mod2.vifs,
     xlab="", ylab="VIF",
     xaxt='n',
     main="VIF Values for Second Model")
abline(h=5, col="blue")

points(which.max(mod2.vifs), max(mod2.vifs), pch=23, col="red", bg="red")
text(which.max(mod2.vifs)-3, max(mod2.vifs)-0.5, label=names(mod2$coefficients[which.max(mod2.vifs)+1]))

```

## Model Coefficients
In this model, the factor (Regionname) Western Metropolitan is the variable with highest VIF value. The model coefficients are shown as follows:


```{r lm2_mod, results='asis'}

mod2.summ <-summary(mod2)
plot(mod2.summ$coefficients[,"Pr(>|t|)"],
     xlab="", ylab="p-value",
     xaxt='n',                 
     main="p-values for Second Model")
abline(h=0.05, col="blue")

points(11:mod2$rank, mod2.summ$coefficients[,"Pr(>|t|)"][11:mod2$rank], pch=21, col="blue", bg="blue")
points(3,mod2.summ$coefficients[,"Pr(>|t|)"][3], pch=23, col="red", bg="red")
text(3, mod2.summ$coefficients[,"Pr(>|t|)"][3]-0.01, label=names(mod2$coefficients[3]))

legend("topright",
       pch=21,
       pt.bg="blue",
       col="blue",
       legend="Region Factor Levels")
```

The p-value of Landsize is the highest, which is larger than 0.05. Thus, this variable should be dropped. 

## Linear Model 3: Considered Interactions
In this model, we analyze the interaction of rooms and region. The result is shown as follows: 


```{r lm3, results='asis'}
#  mod 2 with rooms*region interaction -  failed model, drop interaction
mod3 <-  lm(Price ~ Rooms + Landsize
          + Distance  + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname)*Rooms , data=data.train)


mod3.summ <-summary(mod3)                                                                                          
plot(mod3.summ$coefficients[,"Pr(>|t|)"],
     xlab="", ylab="p-value",                                                                                              
     xaxt='n',
     main="p-values for Model with Interaction")
abline(h=0.05, col="blue")

points(11:17, mod3.summ$coefficients[,"Pr(>|t|)"][11:17], pch=21, col="blue", bg="blue")
points(18:24, mod3.summ$coefficients[,"Pr(>|t|)"][18:24], pch=21, col="darkgreen", bg="darkgreen")

legend("topright",
       pch=21,
       pt.bg=c("blue", "darkgreen"),
       col=c("blue", "darkgreen"),
       legend=c("Region Factor Levels", "Interaction Rooms:Region"))



```

The p-value for model with interaction shows that all of the variables are suitable and acceptable. 


## Linear Model 4: Removed Land Size
From the consequence of model 2, the variable of landsize should be dropped. In this model, the model is set as:
Price ~ Rooms + Distance + Bathroom + Car + BuildingArea + Lattitude + Longtitude + Propertycount + factor (Regionname).


```{r lm4, results='asis'}

mod4 <-  lm(Price ~ Rooms                                                                                
          + Distance  + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname) , data=data.train)



mod4.vifs <- vif(mod4)

plot(mod4.vifs,                                               
     xlab="", ylab="VIF",
     xaxt='n',
     main="VIF Values for Forth Model")
abline(h=5, col="blue")

points(which.max(mod4.vifs), max(mod4.vifs), pch=23, col="red", bg="red")
text(which.max(mod4.vifs)-3, max(mod4.vifs)-0.5, label=names(mod4$coefficients[which.max(mod4.vifs)+1]))
```

From the results of VIF Value for model 4, the factor (Regionname) Western Metropolitan should be dropped. At the same time, the VIF value of this variable is the only variable that exceeds 5.

## Model 4 Coefficients
```{r lm4_pvalues, results='asis'}

mod4.summ <- summary(mod4)

plot(mod4.summ$coefficients[,"Pr(>|t|)"],
     xlab="", ylab="p-value",
     xaxt='n',
     main="p-values for Second Model")
abline(h=0.05, col="blue")                                                  

points(10:mod4$rank, mod4.summ$coefficients[,"Pr(>|t|)"][10:mod4$rank], pch=21, col="blue", bg="blue")


legend("topright",
       pch=21,
       pt.bg="blue",
       col="blue",
       legend="Region Factor Levels")


```
# Residual Analysis

## Homogeneity? No                                                              
```{r lm4_redids}

plot(mod4, which=1, sub.caption="Model 4")
```

## Normal? Not Quite
```{r lm4_qq}

plot(mod4, which=2, sub.caption="Model 4")                                                                                                                                                      
```

## Influence? Yes
```{r lm4_lev}

plot(mod4, which=5, sub.caption="Model 4")
```



## Remove Influence Points
```{r lm5}
                               
# drop drop influence points
mod5 <- mod4 <-  lm(Price ~ Rooms
          + Distance  + Bathroom + Car + BuildingArea
          + Lattitude + Longtitude + Propertycount
          + factor(Regionname), data=data.train[!(rownames(data.train) %in% c("13246","2561", "1589")),])

plot(mod4, which=5, sub.caption="Model 4 without Outliers")
```

                                                                                                       
# Proposed Model 
This final proposed model and details of related coefficients are shown as follows:
``` {r prop_mod, results="markdown"}
summ(mod4)
```
## Testing $R^2$

```{r lm4_pred}
y.pred <- predict(mod4, data.test)                                        
y.test <- data.test$Price                          

diff.sq = (y.pred - y.test)^2 

keeps <- which(!is.na(diff.sq))

r2 <- function(y.predict, y.actual=y.test){
  
  TSS <- sum((y.actual - mean(y.actual))^2)
  RSS <- sum((y.predict - y.actual)^2)
  
  rSq <- 1 - RSS/TSS
  
  return(rSq)
}
    
r.2 <- r2(y.pred[keeps], y.test[keeps])                                           

```

$$ \begin{equation} R^2 = 1- \dfrac{RSS}{TSS} \end{equation}=0.441$$ 






# Conclusion
The previous analysis did not deal with outliers, and the processing of outliers may also have a certain effect on result optimization. Through the analysis of this data set, the content of linear regression was practiced, and the final effect was not bad. For the future, some useful method can be implemented, such as further explore log transformation, consider GLM with log link, what to do about factors with many levels (100’s)? deal with missing data and improve Prediction.

## Future Work
- Further explore log transformation                 
- Consider GLM with log link
- What to do about factors with many levels (100's)? 
- Missing data
- Improve Prediction 

# 8 Bibliography 
Dataset available: https://www.kaggle.com/dansbecker/melbourne-housing-snapshot
Thorne,S. (2019, November 3) How the Australian Property Market Performed in 2017. Retrieved from www.openagent.com.au/blog/how-the-australian-property-market-performed-in-2017#. 
Mansfield, E. R., & Helms, B. P. (1982). Detecting multicollinearity. The American Statistician, 36(3a), 158-160.
Daoud, J. I. (2017, December). Multicollinearity and regression analysis. In Journal of Physics: Conference Series (Vol. 949, No. 1, p. 012009). IOP Publishing.
 

